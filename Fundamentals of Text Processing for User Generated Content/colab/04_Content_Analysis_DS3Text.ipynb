{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04_Content_Analysis_DS3Text.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/steve-wilson/ds32019/blob/master/04_Content_Analysis_DS3Text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APfI_c8B40Vn",
        "colab_type": "text"
      },
      "source": [
        "#Fundamentals of Text Analysis for User Generated Content @ [DS3](https://www.ds3-datascience-polytechnique.fr/)\n",
        "\n",
        "# Part 4: Content Analysis\n",
        "\n",
        "[<- Previous: Data Collection](https://colab.research.google.com/drive/1Jjx7t3cAkNTtCcKP4Qkkp5uOqd8wQTB3)\n",
        "\n",
        "[-> Next: Text Embeddings](https://colab.research.google.com/drive/1kXX_ifuY5cnHqUt9Xt0cmzAp_NiVho9n)\n",
        "\n",
        "Dates: June 27-28, 2019\n",
        "\n",
        "Facilitator: [Steve Wilson](https://steverw.com)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdTajgZhkGWX",
        "colab_type": "text"
      },
      "source": [
        "## Initial Setup\n",
        "\n",
        "- **Run \"Setup\" below first.**\n",
        "\n",
        "    - This will load libraries and download some resources that we'll use throughout the tutorial.\n",
        "\n",
        "    - You will see a message reading \"Done with setup!\" when this process completes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKVEnPi34qj4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Setup (click the \"run\" button to the left) {display-mode: \"form\"}\n",
        "\n",
        "## Setup ##\n",
        "\n",
        "# imports\n",
        "\n",
        "# built-in Python libraries\n",
        "# -------------------------\n",
        "import collections\n",
        "import re\n",
        "import string\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 3rd party libraries\n",
        "# -------------------\n",
        "\n",
        "# Natural Language Toolkit (https://www.nltk.org/)\n",
        "import nltk\n",
        "\n",
        "# download punctuation related NLTK functions\n",
        "# (needed for sent_tokenize())\n",
        "nltk.download('punkt')\n",
        "# download NLKT part-of-speech tagger\n",
        "# (needed for pos_tag())\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "# download wordnet\n",
        "# (needed for lemmatization)\n",
        "nltk.download('wordnet')\n",
        "# download stopword lists\n",
        "# (needed for stopword removal)\n",
        "nltk.download('stopwords')\n",
        "# dictionary of English words\n",
        "nltk.download('words')\n",
        "\n",
        "# numpy: matrix library for Python\n",
        "import numpy as np\n",
        "\n",
        "# Gensim for topic modeling\n",
        "import gensim\n",
        "# for loading data\n",
        "import sklearn.datasets\n",
        "# for LDA visualization\n",
        "!pip install pyLDAvis\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim\n",
        "\n",
        "# for uploading data files\n",
        "from google.colab import files\n",
        "\n",
        "# downloading values lexicon\n",
        "!wget https://raw.githubusercontent.com/steve-wilson/values_lexicon/master/lexicon_1_0/values_lexicon.txt\n",
        "!wget https://raw.githubusercontent.com/steve-wilson/values_lexicon/master/sample_data/subreddits/christian_500.txt\n",
        "!wget https://raw.githubusercontent.com/steve-wilson/values_lexicon/master/sample_data/subreddits/business_500.txt\n",
        "!wget https://raw.githubusercontent.com/steve-wilson/values_lexicon/master/sample_data/subreddits/college_500.txt\n",
        "\n",
        "def text_to_lemma_frequencies(text, remove_stop_words=True):\n",
        "    \n",
        "    # split document into sentences\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    \n",
        "    # create a place to store (word, pos_tag) tuples\n",
        "    words_and_pos_tags = []\n",
        "    \n",
        "    # get all words and pos tags\n",
        "    for sentence in sentences:\n",
        "        words_and_pos_tags += nltk.pos_tag(nltk.word_tokenize(sentence))\n",
        "        \n",
        "    # load the lemmatizer\n",
        "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "    \n",
        "    # lemmatize the words\n",
        "    lemmas = [lemmatizer.lemmatize(word,lookup_pos(pos)) for \\\n",
        "              (word,pos) in words_and_pos_tags]\n",
        "    \n",
        "    # convert to lowercase\n",
        "    lowercase_lemmas = [lemma.lower() for lemma in lemmas]\n",
        "    \n",
        "    # load the stopword list for English\n",
        "    stop_words = set([])\n",
        "    if remove_stop_words:\n",
        "        stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "    \n",
        "    # add punctuation to the set of things to remove\n",
        "    all_removal_tokens = stop_words | set(string.punctuation)\n",
        "    \n",
        "    # bonus: also add some custom double-quote tokens to this set\n",
        "    all_removal_tokens |= set([\"''\",\"``\"])\n",
        "    \n",
        "    # only get lemmas that aren't in these lists\n",
        "    content_lemmas = [lemma for lemma in lowercase_lemmas \\\n",
        "                      if lemma not in all_removal_tokens and \\\n",
        "                      re.match(r\"^\\w+$\",lemma)]\n",
        "    \n",
        "    # return the frequency distribution object\n",
        "    return nltk.probability.FreqDist(content_lemmas)\n",
        "    \n",
        "def docs2matrix(document_list):\n",
        "    \n",
        "    # use the vocab2index idea from before\n",
        "    vocab2index = {}\n",
        "    \n",
        "    # load the stopword list for English\n",
        "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "    stop_words |= set(['from', 'subject', 're', 'edu', 'use'])\n",
        "    \n",
        "    # add punctuation to the set of things to remove\n",
        "    all_removal_tokens = stop_words | set(string.punctuation)\n",
        "    \n",
        "    # bonus: also add some custom double-quote tokens to this set\n",
        "    all_removal_tokens |= set([\"''\",\"``\"])\n",
        "    \n",
        "    vocab2index = {}\n",
        "    latest_index = 0\n",
        "\n",
        "    lfs = []\n",
        "    # this should be a nice starting point\n",
        "    for doc in document_list:\n",
        "        lf = text_to_lemma_frequencies(doc,all_removal_tokens)\n",
        "        for token in lf.keys():\n",
        "            if token not in vocab2index:\n",
        "                vocab2index[token] = latest_index\n",
        "                latest_index += 1\n",
        "                \n",
        "        lfs.append(lf)\n",
        "    \n",
        "    # create the zeros matrix\n",
        "    corpus_matrix = np.zeros((len(lfs), len(vocab2index)))\n",
        "    \n",
        "    for row, lf in enumerate(lfs):\n",
        "        for token, frequency in lf.items():\n",
        "            column = vocab2index[token]\n",
        "            corpus_matrix[row][column] = frequency\n",
        "    \n",
        "    return corpus_matrix, vocab2index\n",
        "\n",
        "    \n",
        "# Lemmatization -- redefining this here to make\n",
        "# code block more self-contained\n",
        "def lookup_pos(pos):\n",
        "    pos_first_char = pos[0].lower()\n",
        "    if pos_first_char in 'nv':\n",
        "        return pos_first_char\n",
        "    else:\n",
        "        return 'n'\n",
        "\n",
        "\n",
        "            \n",
        "print()\n",
        "print(\"Done with setup!\")\n",
        "print(\"If you'd like, you can click the (X) button to the left to clear this output.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJv8G-gS7GId",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "## Content Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_95J7lYM97q9",
        "colab_type": "text"
      },
      "source": [
        "- Now that we have some real data, what are some ways that we can explore what's in it?\n",
        "    - How can we answer the basic question: *What are people talking about in this corpus?*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6xMWv0M7HyH",
        "colab_type": "text"
      },
      "source": [
        "### Topic Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKxxCNGH8o5r",
        "colab_type": "text"
      },
      "source": [
        "- Load a corpus matrix, like the ones we created earlier, into gensim's corpus object:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ERVLeUv7Ka3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this time, let's load all documents in the 20news dataset from these categories\n",
        "categories = ['soc.religion.christian', 'rec.autos', 'talk.politics.misc', \\\n",
        "              'rec.sport.baseball', 'comp.sys.ibm.pc.hardware']\n",
        "newsgroups_train_all = sklearn.datasets.fetch_20newsgroups(subset='train', \\\n",
        "                                              categories=categories).data\n",
        "# using the function we wrote before, but modified to also return the vocab2index\n",
        "corpus_matrix, word2id = docs2matrix(newsgroups_train_all)\n",
        "# reverse this dictionary\n",
        "id2word = {v:k for k,v in word2id.items()}\n",
        "\n",
        "# Dense2Corpus expects that each \n",
        "corpus = gensim.matutils.Dense2Corpus(corpus_matrix, documents_columns=False)\n",
        "print(\"Loaded\",len(corpus),\"documents into a Gensim corpus.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yO59RFk_o2r",
        "colab_type": "text"
      },
      "source": [
        "- Given this, we can run LDA right out of the box:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "025OyjO8_ubT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# As of July 2019, gensim calls a deprecated numpy function and gives lots of warning messages\n",
        "# Let's supress these.\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# run LDA on our corpus, using out dictionary (k=6)\n",
        "lda = gensim.models.LdaModel(corpus, id2word=id2word, num_topics=6)\n",
        "lda.print_topics()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1ZYzpEnVivz",
        "colab_type": "text"
      },
      "source": [
        "- There is still quite a bit of noise in this list because the documents are full of very common words like \"write\", \"subject\", and \"from\".\n",
        "- One common approach is to remove the most (and possibly least) common words before running LDA.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j33bfjEGZesn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_counts = np.sum(corpus_matrix, axis=0)\n",
        "sorted_words = sorted( zip( range(len(total_counts)) ,total_counts), \\\n",
        "                       key=lambda x:x[1], reverse=True )\n",
        "N = 100\n",
        "M = 50\n",
        "top_N_ids = [item[0] for item in sorted_words[:N]]\n",
        "appears_less_than_M_times = [item[0] for item in sorted_words if item[1] < M]\n",
        "vocab_dense = [id2word[idx] for idx in range(len(id2word))]\n",
        "\n",
        "print(\"Top words to remove:\", ' '.join([id2word[idx] for idx in top_N_ids]))\n",
        "\n",
        "remove_indexes = top_N_ids+appears_less_than_M_times\n",
        "corpus_matrix_filtered = np.delete(corpus_matrix,remove_indexes,1)\n",
        "\n",
        "for index in sorted(remove_indexes, reverse=True):\n",
        "    del vocab_dense[index]\n",
        "\n",
        "id2word_filtered = {}\n",
        "word2id_filtered = {}\n",
        "\n",
        "for i,word in enumerate(vocab_dense):\n",
        "    id2word_filtered[i] = word\n",
        "    word2id_filtered[word] = i\n",
        "    \n",
        "corpus_filtered = gensim.matutils.Dense2Corpus(corpus_matrix_filtered, documents_columns=False)\n",
        "\n",
        "print(\"Original matrix shape:\",corpus_matrix.shape)\n",
        "print(\"New matrix shape:\",corpus_matrix_filtered.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNLlfEeEifvb",
        "colab_type": "text"
      },
      "source": [
        "- Now, run LDA again using this new matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zj6Ru2g8igC-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lda = gensim.models.LdaModel(corpus_filtered, id2word=id2word_filtered, num_topics=6)\n",
        "lda.print_topics()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qChk7x4jDbUA",
        "colab_type": "text"
      },
      "source": [
        "- We can also use this model to get topic probabilities for unseen documents:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLEL1uyzDX-i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unseen_doc = \"I went to the baseball game and say the player hit a homerun !\"\n",
        "unseen_doc_bow = [word2id_filtered.get(word.lower(),-1) for word in unseen_doc.split()]\n",
        "unseen_doc_vec = np.zeros(len(word2id_filtered))\n",
        "for word in unseen_doc_bow:\n",
        "    if word >= 0:\n",
        "        unseen_doc_vec[word] += 1\n",
        "unseen_doc_vec = unseen_doc_vec[np.newaxis]\n",
        "unseen_doc_corpus = gensim.matutils.Dense2Corpus(unseen_doc_vec, documents_columns=False)\n",
        "vector = lda[unseen_doc_corpus]  # get topic probability distribution for a document\n",
        "for item in vector:\n",
        "    print(item)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbEN3OK9DRPh",
        "colab_type": "text"
      },
      "source": [
        "- pyLDAvis is a nice tool for visualizing our topics:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RnbNcMGDU-e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pyLDAvis.enable_notebook()\n",
        "\n",
        "# need to create a gensim dictionary object instead of our\n",
        "# lightweight dict object - this is what pyLDA expects as input\n",
        "dictionary = gensim.corpora.Dictionary()\n",
        "dictionary.token2id = word2id_filtered\n",
        "\n",
        "# visualize the LDA model\n",
        "vis = pyLDAvis.gensim.prepare(lda, corpus_filtered, dictionary)\n",
        "vis"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvbkZG1a7LOL",
        "colab_type": "text"
      },
      "source": [
        "### Lexical Resources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "La8mzCFvLaN8",
        "colab_type": "text"
      },
      "source": [
        "- For a fast and easy content analysis, use one of the many available prebuilt dictionaries/lexicons.\n",
        "    - These map words or stems to semantic categories.\n",
        "- We have discussed several lexicons in the slides.\n",
        "- As an example, let's load the lexicon for measuring personal values in text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wFgdmY-7Q3-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_lexicon(lexicon_file_path):\n",
        "    word2cat = collections.defaultdict(list)\n",
        "    with open(lexicon_file_path,'r') as lexicon_file:\n",
        "        for line in lexicon_file:\n",
        "            if line:\n",
        "                word, cat = line.strip().split(\", \")\n",
        "                word2cat[word].append(cat)\n",
        "    return word2cat\n",
        "            \n",
        "values_lexicon = load_lexicon(\"values_lexicon.txt\")\n",
        "print(\"Loaded lexicon with\",len(values_lexicon),\"entries.\")\n",
        "print(\"The categories for 'mother' are:\",values_lexicon['mother'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCHhIR-ILxjY",
        "colab_type": "text"
      },
      "source": [
        "It's very easy to score a document for each category:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXvCQuoWJmnY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_list = [\"christian_500.txt\", \"business_500.txt\", \"college_500.txt\"]\n",
        "\n",
        "for file_name in file_list:\n",
        "    category_counts = collections.defaultdict(int)\n",
        "    \n",
        "    # just look at the first 25K characters\n",
        "    # this way, we don't need to normalize based on the length of the document\n",
        "    # and we'll save some time since this is just for demonstration purposes\n",
        "    text = open(file_name).read().lower()[:25000]\n",
        "    for pattern, categories in values_lexicon.items():\n",
        "        count = re.findall(r'\\b' + pattern + r'\\b', text)\n",
        "        if count:\n",
        "            for category in categories:\n",
        "                category_counts[category] += len(count)\n",
        "    print(file_name,sorted(category_counts.items(),key=lambda x:x[1], reverse=True))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrfyoMOW8Wlc",
        "colab_type": "text"
      },
      "source": [
        "### Putting it together: analyzing social media content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtePbGMpQOSo",
        "colab_type": "text"
      },
      "source": [
        "- Let's try out some content analysis on the corpus that you created.\n",
        "    - Keep in mind any characteristics of the data that may pollute the content, such as \"RT\" at the beginning of a tweet, that you may want to filter out.\n",
        "- Here is some sample code to load a file from your computer. Go ahead and run it to upload your data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCQpn17K8aQH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load your social media data that you created in the previous section.\n",
        "uploaded = files.upload()\n",
        "print(\"Uploaded\",len(uploaded),\"files.\")\n",
        "for filename in uploaded:\n",
        "    print(\"File:\",filename)\n",
        "    # access file data here\n",
        "    with open(filename) as file_handle:\n",
        "        data = file_handle.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HgtKkZbQ0Ek",
        "colab_type": "text"
      },
      "source": [
        "**Exercise 5:** Social Media Analysis\n",
        "\n",
        "- Run LDA on your own data\n",
        "    - Write some code to convert your data into a gensim corpus\n",
        "    - This should involve:\n",
        "        - parsing the input based on the format that you decided upon when you saved the file in the previous section.\n",
        "        - preprocessing the data (you can do this however you like, perhaps differently than we did before -- it's up to you), which might include:\n",
        "            - punctuation removal\n",
        "            - sentence and word tokenization\n",
        "            - cleaning the data (e.g., spelling correction, converting emoji, removing links)\n",
        "            - lemmatization or stemming (if you choose)\n",
        "        - generating a vocab and a count matrix\n",
        "        - creating the gensim corpus object\n",
        "- This one may take a bit more code than the previous exercises.\n",
        "    - It may be a good idea to split your code into separate functions.\n",
        "    - Feel free to search around for documentation or examples. You can use different functions or approaches than we used in the other sections if you find a methodology that you prefer.\n",
        "    - Of course, you can write and run all the code on your own machine if you don't want to do it all in colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46fQs2hQSQwT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the data into a list of documents\n",
        "\n",
        "# preprocess each document\n",
        "\n",
        "# create a gensim corpus object and id2word dictionary\n",
        "\n",
        "corpus = None\n",
        "id2word = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pU_YaXh1L8Nz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Sample Solution (double-click to view) {display-mode: \"form\"}\n",
        "\n",
        "# This is based on the format used to store the output in the \n",
        "# previous sample solution\n",
        "docs = open(\"mytweets.txt\").read().split(\"</TWEET>\\n<TWEET>\")\n",
        "docs[0] = docs[0].replace(\"<TWEET>\",\"\")\n",
        "docs[1] = docs[1].replace(\"</TWEET>\",\"\")\n",
        "\n",
        "# preprocess each document\n",
        "\n",
        "corpus_matrix, word2id = docs2matrix(docs)\n",
        "\n",
        "# create a gensim corpus object and id2word dictionary\n",
        "\n",
        "id2word = {v:k for k,v in word2id.items()}\n",
        "\n",
        "# Dense2Corpus expects that each \n",
        "corpus = gensim.matutils.Dense2Corpus(corpus_matrix, documents_columns=False)\n",
        "print(\"Loaded\",len(corpus),\"documents into a Gensim corpus.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYRQonqMS-0h",
        "colab_type": "text"
      },
      "source": [
        "- Now that you have the corpus object, let's fit and LDA model and visualize the results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HvbTLhlTEI3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Change this, if you like\n",
        "k = 10\n",
        "\n",
        "lda = gensim.models.LdaModel(corpus, id2word=id2word, num_topics=k)\n",
        "lda.print_topics()\n",
        "\n",
        "# need to create a gensim dictionary object instead of our\n",
        "# lightweight dict object - this is what pyLDA expects as input\n",
        "dictionary = gensim.corpora.Dictionary()\n",
        "dictionary.token2id = word2id\n",
        "\n",
        "# visualize the LDA model\n",
        "vis = pyLDAvis.gensim.prepare(lda, corpus, dictionary)\n",
        "vis"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5lqJ1jURAGm",
        "colab_type": "text"
      },
      "source": [
        "- If you still have time, try running the values lexicon on your data.\n",
        "    - Think about how you can use the corpus_matrix that you (may) have already created to easily get the counts of each category for each document.\n",
        "    - Which document has the highest score for each value category?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5uVo6L2RJ7k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Bonus activity workspace\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7ZAPyi4yJrP",
        "colab_type": "text"
      },
      "source": [
        "- [-> Next: Text Embeddings](https://colab.research.google.com/drive/1kXX_ifuY5cnHqUt9Xt0cmzAp_NiVho9n)"
      ]
    }
  ]
}
