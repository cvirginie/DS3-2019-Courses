{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_Text_Processsing_Basics_DS3Text.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/steve-wilson/ds32019/blob/master/01_Text_Processsing_Basics_DS3Text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APfI_c8B40Vn",
        "colab_type": "text"
      },
      "source": [
        "#Fundamentals of Text Analysis for User Generated Content @ [DS3](https://www.ds3-datascience-polytechnique.fr/)\n",
        "\n",
        "# Part 1: Text Processing Basics\n",
        "\n",
        "[-> Next: Noisy Text Processing](https://colab.research.google.com/drive/1VlRz-wKYmsQ4gRHb02uLav8RodpvsCNG)\n",
        "\n",
        "Dates: June 27-28, 2019\n",
        "\n",
        "Facilitator: [Steve Wilson](https://steverw.com)\n",
        "\n",
        "(To edit this notebook: File -> Open in Playground Mode)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdTajgZhkGWX",
        "colab_type": "text"
      },
      "source": [
        "## Initial Setup\n",
        "\n",
        "- **Run \"Setup\" below first.**\n",
        "\n",
        "    - This will load libraries and download some resources that we'll use throughout the tutorial.\n",
        "\n",
        "    - You will see a message reading \"Done with setup!\" when this process completes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKVEnPi34qj4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Setup (click the \"run\" button to the left) {display-mode: \"form\"}\n",
        "\n",
        "## Setup ##\n",
        "\n",
        "# imports\n",
        "\n",
        "# built-in Python libraries\n",
        "# -------------------------\n",
        "\n",
        "# counting and data management\n",
        "import collections\n",
        "# operating system utils\n",
        "import os\n",
        "# regular expressions\n",
        "import re\n",
        "# additional string functions\n",
        "import string\n",
        "# system utilities\n",
        "import sys\n",
        "# request() will be used to load web content\n",
        "import urllib.request\n",
        "\n",
        "\n",
        "# 3rd party libraries\n",
        "# -------------------\n",
        "\n",
        "# Natural Language Toolkit (https://www.nltk.org/)\n",
        "import nltk\n",
        "\n",
        "# download punctuation related NLTK functions\n",
        "# (needed for sent_tokenize())\n",
        "nltk.download('punkt')\n",
        "# download NLKT part-of-speech tagger\n",
        "# (needed for pos_tag())\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "# download wordnet\n",
        "# (needed for lemmatization)\n",
        "nltk.download('wordnet')\n",
        "# download stopword lists\n",
        "# (needed for stopword removal)\n",
        "nltk.download('stopwords')\n",
        "# dictionary of English words\n",
        "nltk.download('words')\n",
        "\n",
        "# numpy: matrix library for Python\n",
        "import numpy as np\n",
        "\n",
        "# scipy: scientific operations\n",
        "# works with numpy objects\n",
        "import scipy\n",
        "\n",
        "# matplotlib (and pyplot) for visualizations\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# sklearn for basic machine learning operations\n",
        "import sklearn\n",
        "import sklearn.manifold\n",
        "import sklearn.cluster\n",
        "\n",
        "# worldcloud tool\n",
        "!pip install wordcloud\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# for checking object memory usage\n",
        "!pip install pympler\n",
        "from pympler import asizeof\n",
        "\n",
        "!pip install spacy\n",
        "import spacy\n",
        "\n",
        "# Downloading data\n",
        "# ----------------\n",
        "if not os.path.exists(\"aclImdb\"):\n",
        "    !wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "    !tar -xzf aclImdb_v1.tar.gz\n",
        "\n",
        "print()\n",
        "print(\"Done with setup!\")\n",
        "print(\"If you'd like, you can click the (X) button to the left to clear this output.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sK9riH96uta",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## A - Basic Text Preprocessing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UepTkw-gjzAn",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### Built-in Python functions\n",
        "\n",
        "- Basic Python fuctions provide a good starting place."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHQ2OyBNtkf7",
        "colab_type": "text"
      },
      "source": [
        "- First, we should try to split a sentence into individual words:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWkjtSPv8H7s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = \"Ã‰cole polytechnique (also known as EP or X) (English: \" + \\\n",
        "       \"Polytechnic School), is a French public institution of higher \"+ \\\n",
        "       \"education and research in Palaiseau, a suburb southwest of Paris.\"\n",
        "\n",
        "# We can split on all whitespace with split()\n",
        "words = text.split()\n",
        "print(\"WORDS:\",words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cdH4AJKBaYx",
        "colab_type": "text"
      },
      "source": [
        "- It is fairly straightforward to do things like remove punctuation, lowercase, or access individual letters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4IKHf5j_nX7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for the first 10 words\n",
        "for word in words [:10]:\n",
        "    \n",
        "    # print the string \"word:\", the word itself, \n",
        "    # and end with a veritcal bar character instead of a newline\n",
        "    print(\"word:\", word, end=' | ')\n",
        "    \n",
        "    # strip removes characters at the beginning and end of a string\n",
        "    # string.punctuation contains: !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
        "    print(\"no punctuation:\", word.strip(string.punctuation), end=' | ')\n",
        "    \n",
        "    # lower() and upper() change case\n",
        "    print(\"lowercase:\", word.lower(), end=' | ')\n",
        "    \n",
        "    # characters in strings can be indexed just like items in lists\n",
        "    print(\"first letter:\", word[0].upper())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KB91dSXxDmWb",
        "colab_type": "text"
      },
      "source": [
        "- How about dealing with multiple sentences?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhOzBcRb_zcg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# From https://en.wikipedia.org/wiki/Data_science\n",
        "\n",
        "text =  'Data science is a \"concept to unify statistics, data analysis, machine ' + \\\n",
        "        'learning and their related methods\" in order to \"understand and analyze ' + \\\n",
        "        'actual phenomena\" with data. '\n",
        "\n",
        "text += 'It employs techniques and theories drawn from many fields within the ' + \\\n",
        "        'context of mathematics, statistics, computer science, and information ' + \\\n",
        "        'science. '\n",
        "\n",
        "text += 'Turing award winner Jim Gray imagined data science as a \"fourth paradigm\"' + \\\n",
        "        'of science (empirical, theoretical, computational and now data-driven) ' + \\\n",
        "        'and asserted that \"everything about science is changing because of the ' + \\\n",
        "        'impact of information technology\" and the data deluge. '\n",
        "\n",
        "text += 'In 2015, the American Statistical Association identified database ' + \\\n",
        "        'management, statistics and machine learning, and distributed and ' + \\\n",
        "        'parallel systems as the three emerging foundational professional communities.\"'\n",
        "\n",
        "# We could try splitting on the period character...\n",
        "sentences = text.split('.')\n",
        "print('.\\n'.join(sentences))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpltocCcioIq",
        "colab_type": "text"
      },
      "source": [
        "- When might this not work?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GsC4WL1Ajkl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Try this:\n",
        "text =  \"Dr. Martin registered the domain name drmartin.com before moving to the \" + \\\n",
        "        \"U.K. in January. \"\n",
        "text += \"During that time, 1.6 million users visited her website... it was very \" + \\\n",
        "        \"unexpected and caused a server to crash.\"\n",
        "sentences = text.split('.')\n",
        "print('.\\n'.join(sentences))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XUVXkhpivUf",
        "colab_type": "text"
      },
      "source": [
        "###Introducing the Natural Language Toolkit (NLTK)\n",
        "\n",
        "- NLTK is a very handy library for basic text processing operations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-B-ShWH3t0AK",
        "colab_type": "text"
      },
      "source": [
        "- We can split sentences in a much smarter way:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Mh_jMl5i2_0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = nltk.sent_tokenize(text)\n",
        "print('\\n'.join(sentences))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBsjCUlbjIO2",
        "colab_type": "text"
      },
      "source": [
        "- **What else can we do with NLTK?**\n",
        "- Smarter word tokenization:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGuVEhjpENNT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence_words = nltk.word_tokenize(sentences[0])\n",
        "print(\"Words:\",' '.join(sentence_words))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U23FpECZUdNq",
        "colab_type": "text"
      },
      "source": [
        "- Finding word stems:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9i2nyLzCEQ-W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Add the words from the 2nd sentence\n",
        "sentence_words += nltk.word_tokenize(sentences[1])\n",
        "\n",
        "# Stemming\n",
        "stemmer = nltk.stem.PorterStemmer()\n",
        "stems = [stemmer.stem(word) for word in sentence_words]\n",
        "print(stems)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JG_RreJUiDA",
        "colab_type": "text"
      },
      "source": [
        "- Labeling words with their part-of-speech, and even finding their lemmas:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NHsxvzYET_D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Part-of-speech tagging\n",
        "pos_tags = nltk.pos_tag(sentence_words)\n",
        "print(\"Parts of speech:\",pos_tags)\n",
        "\n",
        "# Lemmatization\n",
        "def lookup_pos(pos):\n",
        "    pos_first_char = pos[0].lower()\n",
        "    if pos_first_char in 'nv':\n",
        "        return pos_first_char\n",
        "    else:\n",
        "        return 'n'\n",
        "    \n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "lemmas = [lemmatizer.lemmatize(word,lookup_pos(pos)) for (word,pos) in pos_tags]\n",
        "print(\"Lemmas:\", ' '.join(lemmas))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAlWIzCDUqIb",
        "colab_type": "text"
      },
      "source": [
        "- Sometimes, it is helpful to remove \"stopwords\", like \"a, the, I, do,\" and others.\n",
        "    - It's worth thinking about whether or not these words are important in your application.\n",
        "    - These kinds of words do carry a lot of important information!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H95ncKLzEWD7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Stopword (non-content word) removal\n",
        "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "content_words = [word for word in sentence_words if word not in stop_words]\n",
        "removed_stop_words = [word for word in sentence_words if word in stop_words]\n",
        "print(\"Content words:\", ' '.join(content_words))\n",
        "print(\"Removed Stop words:\", ' '.join(removed_stop_words))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pH6yNFYhVDPq",
        "colab_type": "text"
      },
      "source": [
        "- Let's look at a simple plot of the word frequencies in our sample text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPw6RMjNjmT-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get word frequencies\n",
        "frequencies = nltk.probability.FreqDist(sentence_words)\n",
        "\n",
        "# Plot the frequencies\n",
        "frequencies.plot(15,cumulative=False)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLnMR2E1lSea",
        "colab_type": "text"
      },
      "source": [
        "### Putting it together: Creating a Word Cloud\n",
        "- Now, it's your turn to try out some of the techniques we've covered."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSpw7LyFt5VL",
        "colab_type": "text"
      },
      "source": [
        "1. First, run the code block below labeled \"Run this code first\" to perform some setup.\n",
        "2. Then, modify the code marked \"Exercise 1\" to convert a document into **preprocessed lemma frequencies**.\n",
        "    - There is a sample solution below. It's hidden for now, but you can take a peek when you are ready.\n",
        "3. Finally, run the code labeled \"build a word cloud\" to see the result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ldc04mMLHXnT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Run this code first: Wordcloud function and loading the document (double-click to view) {display-mode: \"form\"}\n",
        "\n",
        "\n",
        "# Draw a wordcloud!\n",
        "# Inputs:\n",
        "#   word_counts: a dictionary mapping strings to their counts\n",
        "def draw_wordcloud(freq_dist, colormap):\n",
        "    \n",
        "    #TODO add a few corpus specific checks here to make sure people have done casing, lemmatization, punct removal\n",
        "    uniq_count = len(freq_dist.keys())\n",
        "    print(\"Building a word cloud with\",uniq_count,\"unique words...\")\n",
        "    wc = WordCloud(colormap=colormap, width=1500, \n",
        "                   height=1000).generate_from_frequencies(freq_dist)\n",
        "    plt.imshow(wc, interpolation='bilinear')\n",
        "    plt.axis(\"off\")\n",
        "    \n",
        "print(\"draw_wordcloud() function is ready to use.\")\n",
        "\n",
        "# Load the contents of the book \"The Wonderful Wizard of Oz\" \n",
        "#   by L. Frank Baum (from project Gutenberg)\n",
        "document = urllib.request.urlopen(\"http://www.gutenberg.org/cache/epub/55/pg55.txt\").read().decode('utf-8')\n",
        "\n",
        "print('\"The Wonderful Wizard of Oz\" full text is loaded.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hor-Pa-oIgxb",
        "colab_type": "text"
      },
      "source": [
        "**Exercise 1**\n",
        "\n",
        "Write your code here. Make sure to click the \"run\" button when you're finished."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yiiymar8Ic33",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert text to a dictionary mapping strings to a FreqDist object\n",
        "# containing the frequences of the lemmas in the text.\n",
        "# All stopwords should be removed.\n",
        "# Inputs:\n",
        "#   text: a string as input, possibly containing multiple sentences.\n",
        "def text_to_lemma_frequencies(text):\n",
        "    \n",
        "# ------------- Exercise 1 -------------- #\n",
        "\n",
        "    # write your preprocessing code here\n",
        "\n",
        "    # replace this return function with your own\n",
        "    return nltk.probability.FreqDist([\"Hello\", \"world\", \"hello\", \"world.\"])\n",
        "# ---------------- End ------------------ #\n",
        "\n",
        "    \n",
        "# quick test (do not modify this)\n",
        "test_doc = \"This is a test. Does this work?\"\n",
        "result = text_to_lemma_frequencies(test_doc)\n",
        "passed = result == nltk.probability.FreqDist([\"test\",\"work\"])\n",
        "if passed:\n",
        "    print (\"Test passed!\")\n",
        "else:\n",
        "    print(\"Test did not pass yet.\")\n",
        "    if type(result) == type(nltk.probability.FreqDist([\"a\"])):\n",
        "        print(\"got these words:\", result.keys(),\\\n",
        "              \"\\nwith these counts:\", result.values())\n",
        "    else:\n",
        "        print(\"Did not return a FreqDist object.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJatKHYxR7wt",
        "colab_type": "text"
      },
      "source": [
        "Now, let's **build a word cloud** for the book \"[The Wonderful Wizard of Oz](http://www.gutenberg.org/cache/epub/55/pg55.txt).\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkWmUoEAlZLn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the word frequency distribution\n",
        "freq_dist = text_to_lemma_frequencies(document)\n",
        "\n",
        "# Use default colormap\n",
        "colormap = None\n",
        "# Bonus: try out some other matplotlib colormaps\n",
        "#colormap = \"spring\" # see more here: https://matplotlib.org/3.1.0/tutorials/colors/colormaps.html\n",
        "\n",
        "# Call the function to draw the word cloud\n",
        "draw_wordcloud(freq_dist, colormap)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dED9PJZUp1OR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Sample Solution (double-click to view) Run to load sample solution. {display-mode: \"form\"}\n",
        "\n",
        "def text_to_lemma_frequencies(text, remove_stop_words=True):\n",
        "    \n",
        "    # split document into sentences\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    \n",
        "    # create a place to store (word, pos_tag) tuples\n",
        "    words_and_pos_tags = []\n",
        "    \n",
        "    # get all words and pos tags\n",
        "    for sentence in sentences:\n",
        "        words_and_pos_tags += nltk.pos_tag(nltk.word_tokenize(sentence))\n",
        "        \n",
        "    # load the lemmatizer\n",
        "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "    \n",
        "    # lemmatize the words\n",
        "    lemmas = [lemmatizer.lemmatize(word,lookup_pos(pos)) for \\\n",
        "              (word,pos) in words_and_pos_tags]\n",
        "    \n",
        "    # convert to lowercase\n",
        "    lowercase_lemmas = [lemma.lower() for lemma in lemmas]\n",
        "    \n",
        "    # load the stopword list for English\n",
        "    stop_words = set([])\n",
        "    if remove_stop_words:\n",
        "        stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "    \n",
        "    # add punctuation to the set of things to remove\n",
        "    all_removal_tokens = stop_words | set(string.punctuation)\n",
        "    \n",
        "    # bonus: also add some custom double-quote tokens to this set\n",
        "    all_removal_tokens |= set([\"''\",\"``\"])\n",
        "    \n",
        "    # only get lemmas that aren't in these lists\n",
        "    content_lemmas = [lemma for lemma in lowercase_lemmas \\\n",
        "                      if lemma not in all_removal_tokens]\n",
        "    \n",
        "    # return the frequency distribution object\n",
        "    return nltk.probability.FreqDist(content_lemmas)\n",
        "    \n",
        "# Lemmatization -- redefining this here to make\n",
        "# code block more self-contained\n",
        "def lookup_pos(pos):\n",
        "    pos_first_char = pos[0].lower()\n",
        "    if pos_first_char in 'nv':\n",
        "        return pos_first_char\n",
        "    else:\n",
        "        return 'n'\n",
        "    \n",
        "# quick test:\n",
        "test_doc = \"This is a test. Does this work?\"\n",
        "result = text_to_lemma_frequencies(test_doc)\n",
        "passed = result == nltk.probability.FreqDist([\"test\",\"work\"])\n",
        "if passed:\n",
        "    print (\"Test passed!\")\n",
        "else:\n",
        "    print(\"Test did not pass yet.\")\n",
        "    if type(result) == type(nltk.probability.FreqDist([\"a\"])):\n",
        "        print(\"got these words:\", result.keys(),\\\n",
        "              \"\\nwith these counts:\", result.values())\n",
        "    else:\n",
        "        print(\"Did not return a FreqDist object.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gwsc84RSTT-D",
        "colab_type": "text"
      },
      "source": [
        "### Bonus: Zipf's Law"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liWR6QyiuC4j",
        "colab_type": "text"
      },
      "source": [
        "- Let's check the frequency distribution over the top N words in the book."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDzA0LpYE8P1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "top_n_words = 100\n",
        "freq_dist.plot(top_n_words, cumulative=False)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWEe1E68PelP",
        "colab_type": "text"
      },
      "source": [
        "- You've just observed (a \"Wizard of Oz\" version of) [Zipf's Law](https://en.wikipedia.org/wiki/Zipf%27s_law)  at work!\n",
        "\n",
        "- Remember that we've also removed stopwords. \n",
        "\n",
        "- _Try this_: \n",
        "    - Load the sample `text_to_lemma_frequencies()` function, then run the code below to see what this looks like with stopwords.\n",
        "\n",
        "    - Pay attention to how the y-axis is different from the example above.\n",
        "\n",
        "    - Compare the result to [this example](https://phys.org/news/2017-08-unzipping-zipf-law-solution-century-old.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnxAMXf9SkQG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "freq_dist = text_to_lemma_frequencies(document, remove_stop_words=False)\n",
        "top_n_words = 100\n",
        "freq_dist.plot(top_n_words, cumulative=False)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRm3uG2Gt9oZ",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## B - Corpus-level Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHJ8xHQRTO1a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Skipped part A? Run this cell to load code needed moving forward. {display-mode: \"form\"}\n",
        "\n",
        "print(\"Make sure that you have run 'Initial Setup'!\")\n",
        "# Setup from part 1\n",
        "\n",
        "def text_to_lemma_frequencies(text, remove_stop_words=True):\n",
        "    \n",
        "    # split document into sentences\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    \n",
        "    # create a place to store (word, pos_tag) tuples\n",
        "    words_and_pos_tags = []\n",
        "    \n",
        "    # get all words and pos tags\n",
        "    for sentence in sentences:\n",
        "        words_and_pos_tags += nltk.pos_tag(nltk.word_tokenize(sentence))\n",
        "        \n",
        "    # load the lemmatizer\n",
        "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "    \n",
        "    # lemmatize the words\n",
        "    lemmas = [lemmatizer.lemmatize(word,lookup_pos(pos)) for \\\n",
        "              (word,pos) in words_and_pos_tags]\n",
        "    \n",
        "    # convert to lowercase\n",
        "    lowercase_lemmas = [lemma.lower() for lemma in lemmas]\n",
        "    \n",
        "    # load the stopword list for English\n",
        "    stop_words = set([])\n",
        "    if remove_stop_words:\n",
        "        stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "    \n",
        "    # add punctuation to the set of things to remove\n",
        "    all_removal_tokens = stop_words | set(string.punctuation)\n",
        "    \n",
        "    # bonus: also add some custom double-quote tokens to this set\n",
        "    all_removal_tokens |= set([\"''\",\"``\"])\n",
        "    \n",
        "    # only get lemmas that aren't in these lists\n",
        "    content_lemmas = [lemma for lemma in lowercase_lemmas \\\n",
        "                      if lemma not in all_removal_tokens]\n",
        "    \n",
        "    # return the frequency distribution object\n",
        "    return nltk.probability.FreqDist(content_lemmas)\n",
        "    \n",
        "# Lemmatization -- redefining this here to make\n",
        "# code block more self-contained\n",
        "def lookup_pos(pos):\n",
        "    pos_first_char = pos[0].lower()\n",
        "    if pos_first_char in 'nv':\n",
        "        return pos_first_char\n",
        "    else:\n",
        "        return 'n'\n",
        "    \n",
        "print(\"Otherwise, you're now ready for part 2.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEaT-4UAwXCk",
        "colab_type": "text"
      },
      "source": [
        "### Matrix Representations\n",
        "\n",
        "- Representing documents as vectors of words gets us one step closer to using traditional data science approaches.\n",
        "\n",
        "- However, never forget that we're still working with language data!\n",
        "\n",
        "- **How do we get a corpus matrix?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtgHwyi4uHi3",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "- First, we'll load a small corpus into memory:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvNorDdMxQ8Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from the Stanford Movie Reviews Data: \n",
        "# http://ai.stanford.edu/~amaas/data/sentiment/\n",
        "\n",
        "# we downloaded this during our initial Setup\n",
        "movie_review_dir = \"aclImdb/train/unsup/\"\n",
        "movie_review_files = os.listdir(movie_review_dir)\n",
        "n_movie_reviews = []\n",
        "n = 50\n",
        "for txt_file_path in sorted(movie_review_files, \\\n",
        "                            key=lambda x:int(x.split('_')[0]))[:n]:\n",
        "        full_path = movie_review_dir + txt_file_path\n",
        "        with open(full_path,'r') as txt_file:\n",
        "            n_movie_reviews.append(txt_file.read())\n",
        "            \n",
        "print(\"Loaded\",len(n_movie_reviews),\"movie reviews from the Stanford IMDB \" + \\\n",
        "      \"corpus into memory.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSzT0qoQxhmm",
        "colab_type": "text"
      },
      "source": [
        "- Start by getting a bag-of-words representation for each review.\n",
        "- Then, create a mapping between the full vocabulary and columns for our matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ap8M_6mBwV5f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "review_frequency_distributions = []\n",
        "\n",
        "# process each review, one at a time\n",
        "for review in n_movie_reviews:\n",
        "    \n",
        "    # let's use our function from before\n",
        "    frequencies = text_to_lemma_frequencies(review)\n",
        "    review_frequency_distributions.append(frequencies)\n",
        "\n",
        "# use a dictionary for faster lookup\n",
        "vocab2index = {}\n",
        "latest_index = 0\n",
        "for rfd in review_frequency_distributions:\n",
        "    for token in rfd.keys():\n",
        "        if token not in vocab2index:\n",
        "            vocab2index[token] = latest_index\n",
        "            latest_index += 1\n",
        "    \n",
        "print(\"Built vocab lookup for vocab of size:\",len(vocab2index))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0NkAsWbu0Do",
        "colab_type": "text"
      },
      "source": [
        "- Given the frequencies and this index lookup, we can build a frequency matrix (as a numpy array)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NUN92WVu8zU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make an all-zero numpy array with shape n x v\n",
        "# n = number of documents\n",
        "# v = vocabulary size\n",
        "corpus_matrix = np.zeros((len(review_frequency_distributions), len(vocab2index)))\n",
        "\n",
        "# fill in the numpy array\n",
        "for row, rfd in enumerate(review_frequency_distributions):\n",
        "    for token, frequency in rfd.items():\n",
        "        column = vocab2index[token]\n",
        "        corpus_matrix[row][column] = frequency"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoZ7HKOKzRBX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get some basic information about our matrix\n",
        "def print_matrix_info(m):\n",
        "    print(\"Our corpus matrix is\",m.shape[0],'x',m.shape[1])\n",
        "    print(\"Sparsity is:\",round(float(100 * np.count_nonzero(m))/ \\\n",
        "                           (m.shape[0] * m.shape[1]),2),\"%\")\n",
        "\n",
        "print_matrix_info(corpus_matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koO0_3BSxg_p",
        "colab_type": "text"
      },
      "source": [
        "- Now that we've seen how this works, let's see how some existing Python functions can do the heavy lifting for us.\n",
        "- Scikit learn has some useful feature extraction methods:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpsI7crJxnPu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we can get a similar corpus matrix with just 3 lines of code\n",
        "vectorizer = sklearn.feature_extraction.text.CountVectorizer()\n",
        "sklearn_corpus_data = vectorizer.fit_transform(n_movie_reviews)\n",
        "sklearn_corpus_matrix = sklearn_corpus_data.toarray()\n",
        "\n",
        "# get the feature names (1:1 mapping to the columns in the matrix)\n",
        "print(\"First 10 features:\",vectorizer.get_feature_names()[:10])\n",
        "print()\n",
        "\n",
        "# let's check out the matrix\n",
        "print_matrix_info(sklearn_corpus_matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cphShoYmxvN1",
        "colab_type": "text"
      },
      "source": [
        "- These matrices are typically _very_ sparse.\n",
        "- It's worth considering [different representations](https://docs.scipy.org/doc/scipy/reference/sparse.html) if memory is a concern.\n",
        "    - Save space by only storing nonzero entries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGD18Za9x1Gm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# E.g., using a CSR matrix representation\n",
        "csr_corpus_matrix = scipy.sparse.csr_matrix(corpus_matrix)\n",
        "print(\"Original matrix: using\", asizeof.asizeof(corpus_matrix)/1000,\"kB\")\n",
        "print(\"CSR matrix: using\", asizeof.asizeof(csr_corpus_matrix)/1000,\"kB\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6F8bb6cPuGMM",
        "colab_type": "text"
      },
      "source": [
        "- There will be a trade-off between memory usage and speed of operations.\n",
        "    - consider the strengths and weaknesses of each representation.\n",
        "        - e.g., CSR has fast row-level operations, but slow column-level operations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WeZEZ5pyO5I",
        "colab_type": "text"
      },
      "source": [
        "### Document Retrieval and Similarity\n",
        "\n",
        "- With this matrix, it's very easy to find all documents containing a specific word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRJYmn4WylNW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "search_term = \"funny\"\n",
        "if search_term in vocab2index:\n",
        "    search_index = vocab2index[search_term]\n",
        "    matches = [i for i in range(corpus_matrix.shape[0]) \\\n",
        "           if corpus_matrix[i][search_index]!=0]\n",
        "\n",
        "    # list the documents that contain the search term\n",
        "    print(\"These documents contain '\"+search_term+\"':\",matches)\n",
        "    print()\n",
        "\n",
        "    # show excerpt where this word appears\n",
        "    example_location = n_movie_reviews[matches[0]].find(search_term)\n",
        "    start,end = max(example_location-30,0), min(example_location+30,len(n_movie_reviews[matches[0]]))\n",
        "    print('For example: \"...',n_movie_reviews[matches[0]][start:end],'...\"')\n",
        "    \n",
        "else:\n",
        "    print(search_term,\"isn't in the sample corups.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ar0P-szymQg",
        "colab_type": "text"
      },
      "source": [
        "- We can even use the notion of vector representations to compute the similarity between two documents.\n",
        "\n",
        "    - (we'll talk about more advanced ways to approach this task later in the tutorial)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRGicr44CIGC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "example_docs =[ \"My dog likes to eat vegetables\",\\\n",
        "                \"Your dog likes to eat fruit\",\\\n",
        "                \"The computer is offline\",\\\n",
        "                \"A computer shouldn't be offline\" ]\n",
        "\n",
        "vectorizer = sklearn.feature_extraction.text.CountVectorizer()\n",
        "example_data = vectorizer.fit_transform(example_docs)\n",
        "example_matrix = example_data.toarray()\n",
        "\n",
        "sim_0_1 = 1-scipy.spatial.distance.cosine(example_matrix[0],example_matrix[1])\n",
        "sim_2_3 = 1-scipy.spatial.distance.cosine(example_matrix[2],example_matrix[3])\n",
        "sim_0_2 = 1-scipy.spatial.distance.cosine(example_matrix[0],example_matrix[2])\n",
        "\n",
        "print(\"Similarity between 0 and 1:\",round(sim_0_1,2))\n",
        "print(\"Similarity between 2 and 3:\",round(sim_2_3,2))\n",
        "print(\"Similarity between 0 and 2:\",round(sim_0_2,2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYG5eIG7CKqZ",
        "colab_type": "text"
      },
      "source": [
        "- We can do the same thing with our corpus of movie reviews:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0QOEBZVy0ME",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# choose a document, and find the most \"similar\" other document in the corpus\n",
        "reference_doc = 0\n",
        "ref_doc_vec = corpus_matrix[reference_doc]\n",
        "sim_to_ref_doc = []\n",
        "for row in corpus_matrix:\n",
        "    sim_to_ref_doc.append(1-scipy.spatial.distance.cosine(ref_doc_vec,row))\n",
        "    \n",
        "print(\"similarity scores:\",sim_to_ref_doc)\n",
        "most_similar = sim_to_ref_doc.index(max(sim_to_ref_doc[1:]))\n",
        "print(n_movie_reviews[0])\n",
        "print(\"is most similar to\")\n",
        "print(n_movie_reviews[most_similar])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aWERohPy7wA",
        "colab_type": "text"
      },
      "source": [
        "### Putting it together: Simple Document Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFm9jdcYL_gn",
        "colab_type": "text"
      },
      "source": [
        "- Let's apply the document to matrix idea to do some simple clustering.\n",
        "- First, let's load a dataset that should exhibit some natural groupings based on topic.\n",
        "    - [20news](http://qwone.com/~jason/20Newsgroups/) is classic NLP dataset for document classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9otrBrGIPE-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load 20 newsgroups dataset - just 100 texts from 3 categories\n",
        "categories = ['comp.sys.ibm.pc.hardware', 'rec.sport.baseball']\n",
        "newsgroups_train_all = sklearn.datasets.fetch_20newsgroups(subset='train',\\\n",
        "                                                 categories=categories)\n",
        "newsgroups_train = newsgroups_train_all.data[:100]\n",
        "newsgroups_labels = newsgroups_train_all.target[:100]\n",
        "\n",
        "print(\"Loaded\",len(newsgroups_train),\"documents.\")\n",
        "print(\"Label distribution:\",collections.Counter(newsgroups_labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJWL6_y4MZzO",
        "colab_type": "text"
      },
      "source": [
        "**Exercise 2**\n",
        "\n",
        "- Now, write a function that creates a corpus matrix from a list of strings containing documents.\n",
        "    - We can use the `text_to_lemma_frequencies` that you wrote earlier as a starting point!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkQOyk23Ll1Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ------------- Exercise 2 -------------- #\n",
        "def docs2matrix(document_list):\n",
        "    \n",
        "    # this should be a nice starting point\n",
        "    lemma_freqs = [text_to_lemma_frequencies(doc) for doc in document_list]\n",
        "\n",
        "    # change this to return a 2d numpy array\n",
        "    return None\n",
        "\n",
        "# -------------     End    -------------- #\n",
        "\n",
        "# quick test with first 10 documents\n",
        "X = docs2matrix(newsgroups_train[:10])\n",
        "if type(X) != type(np.zeros([3,3])):\n",
        "    print(\"Did not return a 2d numpy matrix.\")\n",
        "elif X.shape[0] != 10:\n",
        "    print(\"number of rows should be 10, but is\",X.shape[0])\n",
        "else:\n",
        "    print(\"Created a matrix with shape:\",X.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJy4zWGT4uHi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Sample Solution (double-click to view) {display-mode: \"form\"}\n",
        "\n",
        "def docs2matrix(document_list):\n",
        "    \n",
        "    # use the vocab2index idea from before\n",
        "    vocab2index = {}\n",
        "    \n",
        "    # this should be a nice starting point\n",
        "    lemma_freqs = [text_to_lemma_frequencies(doc) for doc in document_list]\n",
        "\n",
        "    latest_index = 0\n",
        "    for lf in lemma_freqs:\n",
        "        for token in lf.keys():\n",
        "            if token not in vocab2index:\n",
        "                vocab2index[token] = latest_index\n",
        "                latest_index += 1\n",
        "    \n",
        "    # create the zeros matrix\n",
        "    corpus_matrix = np.zeros((len(lemma_freqs), len(vocab2index)))\n",
        "    \n",
        "    for row, lf in enumerate(lemma_freqs):\n",
        "        for token, frequency in lf.items():\n",
        "            column = vocab2index[token]\n",
        "            corpus_matrix[row][column] = frequency\n",
        "    \n",
        "    # change this to return a 2d numpy array\n",
        "    return corpus_matrix\n",
        "\n",
        "\n",
        "# quick test with first 10 documents\n",
        "X = docs2matrix(newsgroups_train[:10])\n",
        "if type(X) != type(np.zeros([3,3])):\n",
        "    print(\"Did not return a 2d numpy matrix.\")\n",
        "elif X.shape[0] != 10:\n",
        "    print(\"number of rows should be 10, but is\",X.shape[0])\n",
        "else:\n",
        "    print(\"Created a matrix with shape:\",X.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVt7UvF88njv",
        "colab_type": "text"
      },
      "source": [
        "- Let's visualize the data in 2 dimensions\n",
        "    - We'll use [T-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) to do the dimensionality reduction.\n",
        "    - Each color (red and blue) will represent one of the \"groun truth\" clusters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cegveYxELYR6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# show corpus in 2d\n",
        "\n",
        "X = docs2matrix(newsgroups_train)\n",
        "print(\"Created a matrix with shape:\",X.shape)\n",
        "tsne = sklearn.manifold.TSNE(n_components=2, random_state=1)\n",
        "X_2d = tsne.fit_transform(X)\n",
        "colors = ['r', 'b']\n",
        "target_ids = range(len(categories))\n",
        "for target, c, label in zip(target_ids, colors, categories):\n",
        "    plt.scatter(X_2d[newsgroups_labels == target, 0], X_2d[newsgroups_labels == target, 1], c=c, label=label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoXfEBDndKG-",
        "colab_type": "text"
      },
      "source": [
        "- The groups have a fair degree of overlap. Can kmeans clustering recover them correctly?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIaFvS1-dYM9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Do kmeans clustering\n",
        "\n",
        "kmeans = sklearn.cluster.KMeans(n_clusters=2, random_state=0, algorithm=\"full\").fit(X)\n",
        "clusters = kmeans.labels_\n",
        "\n",
        "for target, c, label in zip(target_ids, colors, categories):\n",
        "    plt.scatter(X_2d[clusters == target, 0], X_2d[clusters == target, 1], c=c, label=label)\n",
        "\n",
        "# out own purity function\n",
        "def compute_average_purity(clusters, labels):\n",
        "    # and computer the cluster purity\n",
        "    cluster_labels = collections.defaultdict(list)\n",
        "    for i in range(len(clusters)):\n",
        "        cluster = clusters[i]\n",
        "        label = labels[i]\n",
        "        cluster_labels[cluster].append(label)\n",
        "    cluster_purities = {}\n",
        "    for cluster, labels in cluster_labels.items():\n",
        "        most_common_count = collections.Counter(labels).most_common()[0][1]\n",
        "        purity = float(most_common_count)/len(labels)\n",
        "        cluster_purities[cluster] = purity\n",
        "    avg_purity = sum(cluster_purities.values())/len(cluster_purities.keys())\n",
        "    print(\"Average cluster purity:\",avg_purity)\n",
        "    \n",
        "avg_purity = compute_average_purity(clusters, newsgroups_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FEqeMWSdYuS",
        "colab_type": "text"
      },
      "source": [
        "- That didn't work as well as we'd like it to.\n",
        "- It's time to introduce better features that just word frequencies.\n",
        "    - TF-IDF to the rescue!\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWWn4Re11dBj",
        "colab_type": "text"
      },
      "source": [
        "### TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLnoPCKVAvrO",
        "colab_type": "text"
      },
      "source": [
        "- Some words are less important when making distinctions between documents in a corpus.\n",
        "- How can we determine the \"less important\" words?\n",
        "    - Using term-frequency * inverse document frequency, we make the assumption that words that appear in *many documents* are *less informative* overall.\n",
        "    - Therefore, we weigh each term based on the inverse of the number of documents that that term appears in.\n",
        "    - We can define $\\operatorname{tfidf}(t,d,D) = \\operatorname{tf}(t,d) * \\log\\frac{|D|}{|d \\in D : t \\in d|}$ , where\n",
        "        - $t$ is a term (token) in a corpus\n",
        "        - $d$ is a document in the corpus\n",
        "        - $D$ is the corpus itself, containing documents, which, in turn, contain tokens\n",
        "        - $\\operatorname{tf}(t,d)$ is the frequency of $t$ in $d$ (typically normalized at the document level).\n",
        "- sklearn has another vectorizer that takes care of this for us: the [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
        "    - It behaves just like the CountVectorizer() that we saw before, except it computes tfidf scores instead of counts!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFcY3Gm-JT41",
        "colab_type": "text"
      },
      "source": [
        "- Of course we can just use the TfidfVectorizer, but what would it look like to implement this ourselves?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KSXAxnL09Dq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# assume input matrix contains term frequencies\n",
        "def tfidf_transform(mat):\n",
        "    \n",
        "    # convert matrix of counts to matrix of normalized frequencies\n",
        "    normalized_mat = mat / np.transpose(mat.sum(axis=1)[np.newaxis])\n",
        "    \n",
        "    # compute IDF scores for each word given the corpus\n",
        "    docs_using_terms = np.count_nonzero(mat,axis=0)\n",
        "    idf_scores = np.log(mat.shape[1]/docs_using_terms)\n",
        "    \n",
        "    # compuite tfidf scores\n",
        "    tfidf_mat = normalized_mat * idf_scores\n",
        "    return tfidf_mat\n",
        "\n",
        "tfidf_X = tfidf_transform(X)\n",
        "print(\"Counts:\",X[0][0:10])\n",
        "print(\"TFIDF scores:\",tfidf_X[0][0:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlbVxUYXJbL0",
        "colab_type": "text"
      },
      "source": [
        "- What happens if we use tfidf instead of just counts or frequencies?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e7YdnPycz_D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# show corpus in 2d\n",
        "\n",
        "#X = docs2matrix(newsgroups_train)\n",
        "vectorizer = sklearn.feature_extraction.text.TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(newsgroups_train).todense()\n",
        "print(\"Created a matrix with shape:\",X.shape)\n",
        "tsne = sklearn.manifold.TSNE(n_components=2, random_state=1)\n",
        "X_2d = tsne.fit_transform(X)\n",
        "colors = ['r', 'b']\n",
        "target_ids = range(len(categories))\n",
        "for target, c, label in zip(target_ids, colors, categories):\n",
        "    plt.scatter(X_2d[newsgroups_labels == target, 0], X_2d[newsgroups_labels == target, 1], c=c, label=label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taQcp0UAJh4g",
        "colab_type": "text"
      },
      "source": [
        "- These groups appear to have a bit more separation.\n",
        "- How well can kmeans recover the original groups now?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4TatTfFzB7K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Do kmeans clustering with TF-IDF matrisx\n",
        "\n",
        "kmeans = sklearn.cluster.KMeans(n_clusters=2, random_state=0, algorithm=\"full\").fit(X)\n",
        "clusters = kmeans.labels_\n",
        "\n",
        "for target, c, label in zip(target_ids, colors, categories):\n",
        "    plt.scatter(X_2d[clusters == target, 0], X_2d[clusters == target, 1], c=c, label=label)\n",
        "    \n",
        "avg_purity = compute_average_purity(clusters, newsgroups_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gARK1cTYWMWU",
        "colab_type": "text"
      },
      "source": [
        "### Bonus: SpaCy\n",
        "- If you have extra time, check out the [SpaCy 101 tutorial](https://spacy.io/usage/spacy-101)!\n",
        "    - SpaCy is less research focused, but after you have a good grasp on the core concepts, it can provide a powerful set of NLP tools, and it is definitely worth knowing about.\n",
        "        - It is also often faster to run than NLTK.\n",
        "        - (we will time our nltk version first, for reference)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HK1p4QRx3b33",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%timeit docs2matrix(newsgroups_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYYxRB_1q9jA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Example preprocessing with SpaCy\n",
        "def text_to_lemma_frequencies(text):\n",
        "    nlp = spacy.load('en')\n",
        "    doc = nlp(text)\n",
        "    words = [token.lemma for token in doc if token.is_stop != True and token.is_punct != True]\n",
        "    return collections.Counter(words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DW4X4ZzwsgFP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Example document matrix building \n",
        "X = docs2matrix(newsgroups_train)\n",
        "print(\"Created a matrix with shape:\",X.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PliAhvcf3Uxl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%timeit docs2matrix(newsgroups_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aC64NI4B4u-M",
        "colab_type": "text"
      },
      "source": [
        "- Why so slow?\n",
        "    - SpaCy is doing too many tasks that we don't need here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AByJBR2y4225",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NLP = spacy.load('en',disable=['ner','parser'])\n",
        "def text_to_lemma_frequencies(text):    \n",
        "    doc = NLP(text)\n",
        "    words = [token.lemma for token in doc if token.is_stop != True and token.is_punct != True]\n",
        "    return collections.Counter(words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzO9KyBr5kgk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%timeit docs2matrix(newsgroups_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRVVvxfwprwY",
        "colab_type": "text"
      },
      "source": [
        "- That's all of the basic text processing that we're going to cover for now.\n",
        "\n",
        "- [-> Next: Noisy Text Processing](https://colab.research.google.com/drive/1VlRz-wKYmsQ4gRHb02uLav8RodpvsCNG)"
      ]
    }
  ]
}
