{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02_Noisy_Text_Processing_DS3Text.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/steve-wilson/ds32019/blob/master/02_Noisy_Text_Processing_DS3Text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APfI_c8B40Vn",
        "colab_type": "text"
      },
      "source": [
        "#Fundamentals of Text Analysis for User Generated Content @ [DS3](https://www.ds3-datascience-polytechnique.fr/)\n",
        "\n",
        "# Part 2: Noisy Text Processing\n",
        "\n",
        "[<- Previous: Text Processing Basics](https://colab.research.google.com/drive/1_RjgWX5FfVeaipdDIS1zHsDUrfYnBT60)\n",
        "\n",
        "[-> Next: Data Collection](https://colab.research.google.com/drive/1Jjx7t3cAkNTtCcKP4Qkkp5uOqd8wQTB3)\n",
        "\n",
        "Dates: June 27-28, 2019\n",
        "\n",
        "Facilitator: [Steve Wilson](https://steverw.com)\n",
        "\n",
        "(To edit this notebook: File -> Open in Playground Mode)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdTajgZhkGWX",
        "colab_type": "text"
      },
      "source": [
        "## Initial Setup\n",
        "\n",
        "- **Run \"Setup\" below first.**\n",
        "\n",
        "    - This will load libraries and download some resources that we'll use throughout the tutorial.\n",
        "\n",
        "    - You will see a message reading \"Done with setup!\" when this process completes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKVEnPi34qj4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Setup (click the \"run\" button to the left) {display-mode: \"form\"}\n",
        "\n",
        "## Setup ##\n",
        "\n",
        "# imports\n",
        "\n",
        "# built-in Python libraries\n",
        "# -------------------------\n",
        "\n",
        "# counting and data management\n",
        "import collections\n",
        "# operating system utils\n",
        "import os\n",
        "# regular expressions\n",
        "import re\n",
        "# additional string functions\n",
        "import string\n",
        "# system utilities\n",
        "import sys\n",
        "# request() will be used to load web content\n",
        "import urllib.request\n",
        "\n",
        "\n",
        "# 3rd party libraries\n",
        "# -------------------\n",
        "\n",
        "# Natural Language Toolkit (https://www.nltk.org/)\n",
        "import nltk\n",
        "\n",
        "# download punctuation related NLTK functions\n",
        "# (needed for sent_tokenize())\n",
        "nltk.download('punkt')\n",
        "# download NLKT part-of-speech tagger\n",
        "# (needed for pos_tag())\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "# download wordnet\n",
        "# (needed for lemmatization)\n",
        "nltk.download('wordnet')\n",
        "# download stopword lists\n",
        "# (needed for stopword removal)\n",
        "nltk.download('stopwords')\n",
        "# dictionary of English words\n",
        "nltk.download('words')\n",
        "\n",
        "# numpy: matrix library for Python\n",
        "import numpy as np\n",
        "\n",
        "# scipy: scientific operations\n",
        "# works with numpy objects\n",
        "import scipy\n",
        "\n",
        "# matplotlib (and pyplot) for visualizations\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# sklearn for basic machine learning operations\n",
        "import sklearn\n",
        "import sklearn.manifold\n",
        "import sklearn.cluster\n",
        "\n",
        "# for spelling correction\n",
        "!pip install pyspellchecker\n",
        "import spellchecker\n",
        "\n",
        "!pip install emoji\n",
        "import emoji\n",
        "\n",
        "!pip install spacy\n",
        "import spacy\n",
        "    \n",
        "# redefine some functions from before\n",
        "NLP = spacy.load('en',disable=['ner','parser'])\n",
        "def text_to_lemma_frequencies(text):\n",
        "    doc = NLP(text)\n",
        "    words = [token.lemma for token in doc if token.is_stop != True and token.is_punct != True]\n",
        "    return collections.Counter(words)\n",
        "    \n",
        "# quick test:\n",
        "test_doc = \"This is a test. Does this work?\"\n",
        "result = text_to_lemma_frequencies(test_doc)\n",
        "passed = result == nltk.probability.FreqDist([\"test\",\"work\"])\n",
        "if passed:\n",
        "    print (\"Test passed!\")\n",
        "else:\n",
        "    print(\"Test did not pass yet.\")\n",
        "    if type(result) == type(nltk.probability.FreqDist([\"a\"])):\n",
        "        print(\"got these words:\", result.keys(),\\\n",
        "              \"\\nwith these counts:\", result.values())\n",
        "    else:\n",
        "        print(\"Did not return a FreqDist object.\")\n",
        "\n",
        "print()\n",
        "print(\"Done with setup!\")\n",
        "print(\"If you'd like, you can click the (X) button to the left to clear this output.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pC-DvU0NzW_O",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## Dealinggg w/ #NoisyText üí¨ ‚û°Ô∏è üíª \n",
        "\n",
        "(depending on your OS/Browswer, emoji may be [displayed differently](https://unicode.org/emoji/charts/full-emoji-list.html))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAw2hjEs0d7f",
        "colab_type": "text"
      },
      "source": [
        "### Spelling Correction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COOwAsuCJvdM",
        "colab_type": "text"
      },
      "source": [
        "- One of the most common isses when dealing with user-generated content is typos or alternate spellings.\n",
        "- Ideally, our text analysis systems will be robust to this sort of thing.\n",
        "- We could:\n",
        "    1. ignore it.\n",
        "    2. use a pre-defined vocabulary and remove any unknown words.\n",
        "    3. use a character based model that will be less sensitive to it.\n",
        "    4. try to automatically correct it.\n",
        "- Let's look at how we might implement the last approach."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Djrp_0yo0g7o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "typo_text = \"This is some sameple text that probabbly contais some typos.\"\n",
        "\n",
        "spellcheck = spellchecker.SpellChecker()\n",
        "corrected_text = [spellcheck.correction(word) for word in typo_text.split()]\n",
        "print(' '.join(corrected_text))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1Q94s9ABdXa",
        "colab_type": "text"
      },
      "source": [
        "- This spellchecker module uses the notion of [Levenshtein Distance](https://en.wikipedia.org/wiki/Levenshtein_distance) between words.\n",
        "    - Other more sophisticated spell checking tools might also look for things like keyboard key distance or lists of common mistakes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQnnMoL-APGn",
        "colab_type": "text"
      },
      "source": [
        "- You may also want to add rules to handle emerging lingistic phenomena such as the repitition of the last (or other) letters of a word\n",
        "    - these kinds of alterations are difficult to handle with edit distance because they may contain many extra characters.\n",
        "    - e.g., \"sorryyyyyy\", \"thanksssss\", or \"byeeeeee\"\n",
        "    - It is worth deciding whether or not this information is worth recording. You could:\n",
        "        - Try to completely remove the additional letters by checking for repititions, removing letters until a match is found in a dictionary.\n",
        "            - This means that \"sorryyyyyy\" and \"sorry\" will be treated as the same word!\n",
        "        - Remove them, but include a token like \\<rep\\> or \\<rep-N\\>at the end of the word to denote N repeats of the preceeding letter.\n",
        "            - This way, sorry can be matched with sorry\\<rep-3\\> if desired, and total repititions per sentence/document/user can be easily computed.\n",
        "        - Leave these words as they are\n",
        "            - This means that \"sorryyyy\" and \"sorryyy\" will be treated as different words!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LNm2cZhEcR9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load a set of English words that we can check against\n",
        "dictionary = set(nltk.corpus.words.words())\n",
        "sentence = \"Ahhhhhh I can't believeee it! Yessssssss\"\n",
        "\n",
        "# let's just check at the end of words for now\n",
        "def normalize_repititions(word, dictionary):\n",
        "    counter = 0\n",
        "    while word not in dictionary and word[-1] == word[-2]:\n",
        "        word = word[:-1]\n",
        "        counter += 1\n",
        "    if counter:\n",
        "        return word + \" <rep-\" + str(counter) + '>'\n",
        "    else:\n",
        "        return word\n",
        "    \n",
        "print(\" \".join([normalize_repititions(word,dictionary) for word in sentence.split()]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_2BZmpXz6fF",
        "colab_type": "text"
      },
      "source": [
        "### When special characters matter ¬Ø\\\\_(„ÉÑ)_/¬Ø "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qa1ARR6j8191",
        "colab_type": "text"
      },
      "source": [
        "- Before, we just removed punctuation marks and special characters.\n",
        "    - Like many of the decisions we are making, there is a time when this could be desirable, but what about when dealing with user-generated content?\n",
        "    - Special characters often contain meaningful semantic content, so they might be worth keeping.\n",
        "- One popular example is [emoji](http://www.unicode.org/emoji/charts/full-emoji-list.html):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLjTlz5_1VuL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use the Python emoji library\n",
        "print(emoji.emojize(\"We're at DS3 in :France:!\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2osfxdcV_wUn",
        "colab_type": "text"
      },
      "source": [
        "- You can check the exact names of the emoji [here](https://github.com/carpedm20/emoji/blob/master/emoji/unicode_codes.py).\n",
        "- Perhaps more useful for us is going the other direction:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQqLkECt_8kd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(emoji.demojize('I just saw the new Avengers movie üëçüëçüëç'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuXrPucqAI0W",
        "colab_type": "text"
      },
      "source": [
        "- Consider how the meaning of that sentence would change given different emoji.\n",
        "- By demojizing our input, we can retain the emoji information and still continue to process our data as plain text.\n",
        "    - *Note:* It's still possible to read and handle the emoji themselves, but it will be much easier to do things like `the_emoji==\":thumbs_up:\"` in our code if we don't have an easy way to type emoji (and are left copy-pasting everything)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5a13fSWyEoo",
        "colab_type": "text"
      },
      "source": [
        "œûœû(‡πë‚öà ‚Ä§Ã´ ‚öà‡πë)‚à©\n",
        "\n",
        "- We also might be interested in emoji's predecessor: [emoticons](https://en.wikipedia.org/wiki/List_of_emoticons)\n",
        "    - however, it has been shown that the inclusion of more emoji causes a decrease in emoticon usage on platforms like Twitter\n",
        "- How could we handle these?\n",
        "    1. Having a dictionary of these can be useful, since we can't necessarily consider any random combination of non-alphanumeric characters to be an emoticon.\n",
        "    2. Alternatively, we could try to [write regular expressions](https://github.com/aritter/twitter_nlp/blob/master/python/emoticons.py) to detect the typical \\<eyes\\> (\\<nose\\>) \\<mouth\\> structure in typical Western emoticons\n",
        "    3. A third option would be to learn from the data: we can build a list of non-alphanumeric sequences that appear with at least some predefined frequency.\n",
        "- People are creative! New combinations are appearing every day, even [mixing emoji with other characters](https://www.fastemoji.com/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rz1qeGC0pAE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# find possible emoticons\n",
        "\n",
        "def find_nonalphanum_seqs(text):\n",
        "    return re.findall(r\"[^\\w\\d\\s]{2,}\",text)\n",
        "\n",
        "text = \"There's a emoticon in this sentence Ôºº(Ôºæ=Ôºæ)Ôºè can you find it?\"\n",
        "print(find_nonalphanum_seqs(text))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDLtEM3C2dDI",
        "colab_type": "text"
      },
      "source": [
        "- Even the above approach has issues because of the letter **o** in the middle of the celebrating emoticon.\n",
        "    - There is no \"rule\" on what can be an emoticon\n",
        "        - it is actually based on the visual resemblance between a sequence of characters and some real-world expression/object."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tl7hd9we0E1j",
        "colab_type": "text"
      },
      "source": [
        "### Social elements of user-generated content\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPggOLpABGvo",
        "colab_type": "text"
      },
      "source": [
        "- Language is inherently social.\n",
        "- User-generated content, especially on social media platforms, often makes the social component of the text more explicit.\n",
        "    - We should try to leverage this information whenever possible!\n",
        "- Take the following example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sBSneEp1XMk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a_tweet = \"What word do you first think of when you hear the word ‚Äúbath‚Äù? \" +\\\n",
        "          \"@radamihalcea from @michigan_AI presents work at #NAACL19 \" +\\\n",
        "          \"#NLPCSS showing how your culture and gender are associated with \" +\\\n",
        "          \"your responses to these kinds of questions, and how #NLProc models \" +\\\n",
        "          \"can capture this phenomenon.\"\n",
        "\n",
        "mentions = re.findall(\"\\@\\w+\",a_tweet)\n",
        "hashtags = re.findall(\"\\#\\w+\",a_tweet)\n",
        "\n",
        "print(\"mentions:\",mentions)\n",
        "print(\"hashtags:\",hashtags)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOZiCf8HCtrr",
        "colab_type": "text"
      },
      "source": [
        "- Even with *only the text data*, we can extract some meaningful information about the social context of this text.\n",
        "    - We could do even more with metadata like likes, retweets, timestamp, location, user-level information, images, etc.\n",
        "        - That is all available from the Twitter API, and we'll cover that in more detail in the next section.\n",
        "- We can also analyze links (and follow them for even more content),  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srN6fKx88fYS",
        "colab_type": "text"
      },
      "source": [
        "### Putting it together: noisy text processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiLfOcTz4JK0",
        "colab_type": "text"
      },
      "source": [
        "**Exercise 3**: Noisy text processor\n",
        "\n",
        "- Write a fuction that takes a sentence as input and produces a \"standard text processing\" friendly output. It should (at a minimum)\n",
        "    - convert emoji to strings describing them,\n",
        "    - correct small typos,\n",
        "    - remove standard puctution like sentence ending '.', but keep possible emoticons.\n",
        "- As a bonus, you could:\n",
        "    - convert between American and British English spellings\n",
        "        - collapse into one spelling per word, but add a tag to the beginng or end of the sentence indicating which dialect was used\n",
        "    - split hashtags into meaningful word sequences\n",
        "        - use capitalization if possible, but use word-matching otherwise\n",
        "            - e.g., `#wordsoftheday` should become something like: `\\<#\\> words of the day \\<\\#\\>`. (example [from Jack Reuter](http://cs.uccs.edu/~jkalita/work/reu/REU2015/FinalPapers/05Reuter.pdf))\n",
        "    - in addition to the cleaned sentence, out a dictionary of a useful features that could be passed to a machine learning model, like \"mentions\", \"possible emoticons\", etc.\n",
        "    - replace character repitions found anywhere in words (not just as the end)\n",
        "        - e.g., converting \"Oh wooooooow that's craaaaazyyyy\" -> \"Oh wow that's crazy\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGO2ZoCqUPl5",
        "colab_type": "text"
      },
      "source": [
        "- Note: These (real) example tweets used come from the [Kaggle Twitter Airline Sentiment Dataset](https://www.kaggle.com/crowdflower/twitter-airline-sentiment/downloads/twitter-airline-sentiment.zip/2)\n",
        "    - the handles of airlines have been replaced with `@airline` so as to avoid expressing opinions about specific companies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrP6NNPY89ml",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_text(noisy_text):\n",
        "    \n",
        "# ------------- Exercise 3 -------------- #\n",
        "\n",
        "    cleaned_text = noisy_text\n",
        "\n",
        "# ---------------- End ------------------ #\n",
        "\n",
        "    return cleaned_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhfu0I9iYcrH",
        "colab_type": "text"
      },
      "source": [
        "- Use the following cell to test your function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmPhYqHKYS9h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from the\n",
        "tweets =  [\"@airline disappointed that u didnt honor my $100 credit given to me \" +\\\n",
        "          \"for ur mistakes. Taking my business elsewhere  ‚úåÔ∏è out.\", \\\n",
        "          \"@airline Awwweesssooomee!\", \\\n",
        "          \"@airline on hold for TWO HOURS now, pick up the PHONEEEEE\", \\\n",
        "          \"@airline I never made a reward reservation becuase no one ever \" +\\\n",
        "          \"answered the phone. The online one I made got Cancelled Flighted \" +\\\n",
        "          \"and I can't change\", \\\n",
        "          \"@airline I will. Thank you for at least tweeting me back :) better \" +\\\n",
        "          \"than most. üëå\"]\n",
        "\n",
        "cleaned = []\n",
        "for tweet in tweets:\n",
        "    cleaned.append(clean_text(tweet))\n",
        "    \n",
        "print(\"Cleaned tweets:\\n\" + '\\n'.join(cleaned))\n",
        "\n",
        "# quick test\n",
        "passed = True\n",
        "print(\"\\nTesting...\")\n",
        "if \"‚úåÔ∏è\" in cleaned[0]:\n",
        "    passed = False\n",
        "    print(\"Did not remove emoji.\")\n",
        "if re.match(\"ee\\b\",cleaned[1]):\n",
        "    passed = False\n",
        "    print(\"Did not remove repititions at word endings.\")\n",
        "if \"becuase\" in cleaned[3]:\n",
        "    passed = False\n",
        "    print(\"Did not correct spelling\")\n",
        "if passed:\n",
        "    print(\"Passed all tests.\")\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nc7UB4ZZlkN9",
        "colab_type": "text"
      },
      "source": [
        "- From the results, you should start to get a feel for how tricky it can be to work with this kind of data.\n",
        "    - For example, the spell checking dictionary doesn't have an entry for \"tweeting\", which leads to an erroneous correction.\n",
        "- Every type of cleaning is a design decision and forces a tradeoff between *natural text* and *machine-friendly text*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGdbqkFpyG-x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Sample Solution (double-click to view) {display-mode: \"form\"}\n",
        "\n",
        "def clean_text(noisy_text):\n",
        "    \n",
        "# ------------- Exercise 3 -------------- #\n",
        "    # creat a spellchecker object\n",
        "    spellcheck = spellchecker.SpellChecker()\n",
        "    # load dictionary of english words\n",
        "    dictionary = set(nltk.corpus.words.words())\n",
        "    \n",
        "    cleaned_words = []\n",
        "    # get sentences\n",
        "    sentences = nltk.sent_tokenize(noisy_text)\n",
        "    for sentence in sentences:        \n",
        "        \n",
        "        demojified = emoji.demojize(sentence)\n",
        "        # and get words\n",
        "        words = nltk.word_tokenize(demojified) \n",
        "        is_emoji = False\n",
        "        for word in words:\n",
        "            \n",
        "            # correct word-level spelling\n",
        "            corrected = word\n",
        "            \n",
        "            # don't spell correct emoji\n",
        "            if not is_emoji:\n",
        "                corrected = spellcheck.correction(word)\n",
        "                     \n",
        "            # remove repititions\n",
        "            counter = 0\n",
        "            if len(corrected) > 2:\n",
        "                while corrected not in dictionary and corrected[-1] == corrected[-2]:\n",
        "                    corrected = corrected[:-1]\n",
        "                    counter += 1\n",
        "            if counter:\n",
        "                cleaned_words += [corrected,\" <rep-\" + str(counter) + '>']\n",
        "            else:\n",
        "                cleaned_words += [corrected]\n",
        "                \n",
        "            is_emoji = word == \":\"\n",
        "                \n",
        "    cleaned_text = ' '.join(cleaned_words)\n",
        "\n",
        "# ---------------- End ------------------ #\n",
        "\n",
        "    return cleaned_text\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxz8xk_WmjwH",
        "colab_type": "text"
      },
      "source": [
        "- Next, we will look at how we can collect some (potentially noisy) text data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdhbYVli7yph",
        "colab_type": "text"
      },
      "source": [
        "- [-> Next: Data Collection](https://colab.research.google.com/drive/1Jjx7t3cAkNTtCcKP4Qkkp5uOqd8wQTB3)"
      ]
    }
  ]
}
