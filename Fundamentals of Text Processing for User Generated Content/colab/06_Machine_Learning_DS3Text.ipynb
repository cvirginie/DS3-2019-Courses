{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "06_Machine_Learning_DS3Text.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/steve-wilson/ds32019/blob/master/06_Machine_Learning_DS3Text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APfI_c8B40Vn",
        "colab_type": "text"
      },
      "source": [
        "#Fundamentals of Text Analysis for User Generated Content @ [DS3](https://www.ds3-datascience-polytechnique.fr/)\n",
        "\n",
        "# Part 6: Machine Learning\n",
        "\n",
        "## Currently Under Construction!\n",
        "## If you are seeing this, you aren't viewing final version of this notebook.\n",
        "## Please check back later\n",
        "\n",
        "[<- Previous: Text Embeddings](https://colab.research.google.com/drive/1kXX_ifuY5cnHqUt9Xt0cmzAp_NiVho9n)\n",
        "\n",
        "Dates: June 27-28, 2019\n",
        "\n",
        "Facilitator: [Steve Wilson](https://steverw.com)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdTajgZhkGWX",
        "colab_type": "text"
      },
      "source": [
        "## Initial Setup\n",
        "\n",
        "- **Run \"Setup\" below first.**\n",
        "\n",
        "    - This will load libraries and download some resources that we'll use throughout the tutorial.\n",
        "\n",
        "    - You will see a message reading \"Done with setup!\" when this process completes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKVEnPi34qj4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Setup (click the \"run\" button to the left) {display-mode: \"form\"}\n",
        "\n",
        "## Setup ##\n",
        "\n",
        "# imports\n",
        "\n",
        "# built-in Python libraries\n",
        "# -------------------------\n",
        "\n",
        "# counting and data management\n",
        "import collections\n",
        "# for dealing with reddit data\n",
        "import json\n",
        "# operating system utils\n",
        "import os\n",
        "# randomization functions\n",
        "import random\n",
        "# regular expressions\n",
        "import re\n",
        "# additional string functions\n",
        "import string\n",
        "# system utilities\n",
        "import sys\n",
        "# request() will be used to load web content\n",
        "import urllib.request\n",
        "\n",
        "\n",
        "# 3rd party libraries\n",
        "# -------------------\n",
        "\n",
        "# Natural Language Toolkit (https://www.nltk.org/)\n",
        "import nltk\n",
        "\n",
        "# download punctuation related NLTK functions\n",
        "# (needed for sent_tokenize())\n",
        "nltk.download('punkt')\n",
        "# download NLKT part-of-speech tagger\n",
        "# (needed for pos_tag())\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "# download wordnet\n",
        "# (needed for lemmatization)\n",
        "nltk.download('wordnet')\n",
        "# download stopword lists\n",
        "# (needed for stopword removal)\n",
        "nltk.download('stopwords')\n",
        "# dictionary of English words\n",
        "nltk.download('words')\n",
        "\n",
        "# numpy: matrix library for Python\n",
        "import numpy as np\n",
        "\n",
        "# scipy: scientific operations\n",
        "# works with numpy objects\n",
        "import scipy\n",
        "\n",
        "# matplotlib (and pyplot) for visualizations\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# sklearn for basic machine learning operations\n",
        "import sklearn\n",
        "import sklearn.manifold\n",
        "import sklearn.cluster\n",
        "\n",
        "# worldcloud tool\n",
        "!pip install wordcloud\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# for checking object memory usage\n",
        "!pip install pympler\n",
        "from pympler import asizeof\n",
        "\n",
        "# for spelling correction\n",
        "!pip install pyspellchecker\n",
        "import spellchecker\n",
        "\n",
        "!pip install spacy\n",
        "import spacy\n",
        "NLP = spacy.load('en',disable=['ner','parser'])\n",
        "\n",
        "!pip install fasttext\n",
        "import fasttext\n",
        "\n",
        "# re-defining some (slightly modified) functions from earlier\n",
        "# -----------------------------------------------------------\n",
        "def text_to_lemma_frequencies(text):    \n",
        "    doc = NLP(text)\n",
        "    words = [token.lemma_ for token in doc if token.is_stop != True and token.is_punct != True]\n",
        "    return collections.Counter(words)\n",
        "\n",
        "# modified version to save memory usage\n",
        "def docs2matrix_sparse(document_list, vocab2index={}, allow_vocab_update=True):\n",
        "    \n",
        "    latest_index = 0\n",
        "    vocab_was_set = False\n",
        "    if vocab2index:\n",
        "        latest_index = max(vocab2index.values()) + 1\n",
        "        vocab_was_set = True\n",
        "    \n",
        "    # make coordinates\n",
        "    data = []\n",
        "    rows = []\n",
        "    cols = []\n",
        "    for row,doc in enumerate(document_list):\n",
        "        lf = text_to_lemma_frequencies(doc)\n",
        "        for token, count in lf.items():\n",
        "            if token not in vocab2index and allow_vocab_update:\n",
        "                vocab2index[token] = latest_index\n",
        "                latest_index += 1\n",
        "            if token in vocab2index:\n",
        "                col = vocab2index[token]\n",
        "                data.append(count)\n",
        "                rows.append(row)\n",
        "                cols.append(col)\n",
        "    \n",
        "    corpus_matrix = None\n",
        "    if vocab_was_set:\n",
        "        max_row = max(rows)\n",
        "        max_col = max(cols)\n",
        "        corpus_matrix = scipy.sparse.coo_matrix( (data, (rows,cols)), (max_row+1 ,latest_index))\n",
        "    else:\n",
        "        # create a coordinate format sparse matrix\n",
        "        corpus_matrix = scipy.sparse.coo_matrix( (data, (rows,cols)))\n",
        "    return corpus_matrix, vocab2index\n",
        "\n",
        "# Downloading data\n",
        "# ----------------\n",
        "if not os.path.exists(\"aclImdb\"):\n",
        "    !wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "    !tar -xzf aclImdb_v1.tar.gz\n",
        "    \n",
        "if not os.path.exists(\"crawl-300d-2M-subword.zip\"):\n",
        "    !wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M-subword.zip\n",
        "else:\n",
        "    print(\"Fasttext embeddings already downloaded\")\n",
        "if not os.path.exists(\"crawl-300d-2M-subword.vec\"):\n",
        "    print(\"Extracting Fasttext embeddings. This may take several minutes...\")\n",
        "    !unzip crawl-300d-2M-subword.zip\n",
        "else:\n",
        "    print(\"Fasttext already extracted.\")\n",
        "if not os.path.exists(\"Stsbenchmark.tar.gz\"):\n",
        "    !wget http://ixa2.si.ehu.es/stswiki/images/4/48/Stsbenchmark.tar.gz\n",
        "    !tar -xzf Stsbenchmark.tar.gz\n",
        "# get sample reddit data\n",
        "if not os.path.exists(\"reddit_2019_05_5K.json\"):\n",
        "    !wget https://raw.githubusercontent.com/steve-wilson/ds32019/master/data/reddit_2019_05_5K.json\n",
        "\n",
        "print()\n",
        "print(\"Done with setup!\")\n",
        "print(\"If you'd like, you can click the (X) button to the left to clear this output.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwSc0l7X9gCv",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## Supervised Learning with Text Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbcUlZ7jIDef",
        "colab_type": "text"
      },
      "source": [
        "- Yes, we are finally going to do some machine learning!\n",
        "- But, before that, we need to setup our data..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7KfP3HMCees",
        "colab_type": "text"
      },
      "source": [
        "### Setting up the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dA00F77NiuY6",
        "colab_type": "text"
      },
      "source": [
        "- Load some sample text data - let's use a portion of the IMDB Reviews that we looked it in part 1.\n",
        "- We will be using the IMDB *test* data as our *development* data for this example notebook. \n",
        "    - Note that in practice, you should always keep a held out test set which is only used for reporting your final results.\n",
        "        - Do not update your model or try any new hyperparamter settings after seeing these results.\n",
        "        - Read more about this idea [here](https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets) if you aren't familiar with machine learning practices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRg5QFWbiBpD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_files(data_dir, amount):\n",
        "    data = []\n",
        "    for file_name in os.listdir(data_dir)[:int(amount/2)]:\n",
        "        with open(data_dir + os.sep + file_name) as file_handle:\n",
        "            file_data = file_handle.read()\n",
        "            data.append(file_data)\n",
        "    return data\n",
        "\n",
        "# setup directories\n",
        "base_dir = \"aclImdb/\"\n",
        "labels = ['pos','neg']\n",
        "# set the amount of files to use for train and test\n",
        "# need to be divisible by 2 to make balanced classes\n",
        "splits_and_amounts = {'train':5000,'test':500}\n",
        "data = {}\n",
        "for split_name, amount in splits_and_amounts.items():\n",
        "    data[split_name] = {}\n",
        "    for label in labels:\n",
        "        data_dir = base_dir + split_name + os.sep + label\n",
        "        data[split_name][label] = read_files(data_dir, amount)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kn2EqyG_yBwT",
        "colab_type": "text"
      },
      "source": [
        "- Now we will load these into *sparse* matrices (`scipy.sparse.coo_matrix`)\n",
        "    - remember from notebook 01 that this will save quite a bit of RAM.\n",
        "- This will take some time now that we're working with a bit more data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nW22MGe6Cjpf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# setup train and test data\n",
        "X_train, vocab = docs2matrix_sparse(data['train']['pos'] + data['train']['neg'])\n",
        "X_test, vocab = docs2matrix_sparse(data['test']['pos'] + data['test']['neg'], vocab, False)\n",
        "y_train = np.array([0]*len(data['train']['pos']) + [1]*len(data['train']['neg']))\n",
        "y_test = np.array([0]*len(data['test']['pos']) + [1]*len(data['test']['neg']))\n",
        "index2vocab = {v:k for k,v in vocab.items()}\n",
        "\n",
        "print(\"X_train shape:\",X_train.shape)\n",
        "print(\"X_test shape:\",X_test.shape)\n",
        "print(\"y_train shape:\",y_train.shape)\n",
        "print(\"y_test shape:\",y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JR0b8s_3ylGe",
        "colab_type": "text"
      },
      "source": [
        "- Next, we will shuffle the data so that we don't pass *all* of the examples from class 1 before class 2.\n",
        "    - Since coo matrices don't support the same type of slicing that dense matrices do, we can use our own function to shuffle the rows.\n",
        "- The shapes of all of the matrices should stay the same after shuffling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8ULlX29fVX_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def coo_sort(order, coo_matrix):\n",
        "    order_lookup = {o:i for i,o in enumerate(order)}\n",
        "    coo_matrix.row = np.array([order_lookup[r] for r in coo_matrix.row])\n",
        "    return coo_matrix\n",
        "\n",
        "# shuffle_data\n",
        "train_reorder = list(range(X_train.shape[0]))\n",
        "test_reorder = list(range(X_test.shape[0]))\n",
        "random.shuffle(train_reorder)\n",
        "random.shuffle(test_reorder)\n",
        "\n",
        "X_train = coo_sort(train_reorder, X_train)\n",
        "X_test = coo_sort(test_reorder, X_test)\n",
        "y_train = y_train[train_reorder]\n",
        "y_test = y_test[test_reorder]\n",
        "\n",
        "print(\"X_train shape:\",X_train.shape)\n",
        "print(\"X_test shape:\",X_test.shape)\n",
        "print(\"y_train shape:\",y_train.shape)\n",
        "print(\"y_test shape:\",y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cVsp8BCCkdi",
        "colab_type": "text"
      },
      "source": [
        "### Logistic Regression for Movie Review Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YC63TX4hzAZs",
        "colab_type": "text"
      },
      "source": [
        "- Let's start with a simple logistic regression classifier, using the word counts as our features:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKwWeB5XCpFG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Using the sklearn LogisticRegression classifier\n",
        "model = sklearn.linear_model.LogisticRegression(solver='lbfgs', max_iter=1000).fit(X_train, y_train)\n",
        "train_preds = model.predict(X_train)\n",
        "test_preds = model.predict(X_test)\n",
        "\n",
        "train_accuracy = sklearn.metrics.accuracy_score(y_train,train_preds)\n",
        "test_accuracy = sklearn.metrics.accuracy_score(y_test,test_preds)\n",
        "print(\"Training accuracy:\",train_accuracy,\"Testing accuracy:\",test_accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4vWLwZDwYyO",
        "colab_type": "text"
      },
      "source": [
        "- Let's try improving this by doing 2 things we've looked at before:\n",
        "    - Removing rare words (let's say, those that appear < 5 times in the training data).\n",
        "    - Using tf-idf scores instead of counts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gx2WbqupzJ3o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# need to include X_test so we can remove the same columns from it\n",
        "# let's convert to csc matrix to do this remove more easily\n",
        "def remove_rare_words(X_train, X_test, index2vocab, threshold = 5):\n",
        "    word_counts = np.sum(X_train, axis=0)\n",
        "    cols_to_keep = [idx for idx in range(word_counts.shape[1]) if word_counts[0, idx] > threshold]\n",
        "    return X_train[:,cols_to_keep], X_test[:,cols_to_keep], update_vocab(index2vocab, cols_to_keep)\n",
        "    \n",
        "def update_vocab(i2v, keep):\n",
        "    keep = set(keep)\n",
        "    list_i2v = [i2v[i] for i in range(len(i2v)) if i in keep]\n",
        "    return {j:v for j,v in enumerate(list_i2v)}\n",
        "    \n",
        "X_train = scipy.sparse.csc_matrix(X_train)\n",
        "X_test = scipy.sparse.csc_matrix(X_test)\n",
        "no_rare_X_train, no_rare_X_test, no_rare_index2vocab = remove_rare_words(X_train, X_test, index2vocab)\n",
        "print(\"X_train shape:\",no_rare_X_train.shape)\n",
        "print(\"X_test shape:\",no_rare_X_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oVWi8AE515v",
        "colab_type": "text"
      },
      "source": [
        "- This reduced our number of features drastically. Let's see how this effects our predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvJ1j2976Etg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = sklearn.linear_model.LogisticRegression(solver='lbfgs', max_iter=1000).fit(no_rare_X_train, y_train)\n",
        "train_preds = model.predict(no_rare_X_train)\n",
        "test_preds = model.predict(no_rare_X_test)\n",
        "\n",
        "train_accuracy = sklearn.metrics.accuracy_score(y_train,train_preds)\n",
        "test_accuracy = sklearn.metrics.accuracy_score(y_test,test_preds)\n",
        "print(\"Training accuracy:\",train_accuracy,\"Testing accuracy:\",test_accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZipNT3UB7YiD",
        "colab_type": "text"
      },
      "source": [
        "- There doesn't seem to have much of an effect, but it seems that we can do just as well without all of those rare words.\n",
        "- Let's add tf-idf to this (this should actually give us some accuracy boost):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umkcyla37n9Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we should only use the IDF scores from X_train to transform both X_train and X_test, \n",
        "# since we can't assume knowledge of X_test while training our model.\n",
        "def tfidf_transform(X_train, X_test):\n",
        "    \n",
        "    # compute IDF scores for each word given X_train\n",
        "    docs_using_terms = np.array(sorted([v for k,v in collections.Counter(X_train.nonzero()[1]).items()]))\n",
        "    idf_scores = np.log(X_train.shape[1]/docs_using_terms)\n",
        "    \n",
        "    # replace 0's with 1's to avoid divide by zero error later\n",
        "    X_train_sum = X_train.sum(axis=1)\n",
        "    X_test_sum = X_test.sum(axis=1)\n",
        "    X_train_sum[np.where(X_train_sum==0)] = 1\n",
        "    X_test_sum[np.where(X_test_sum==0)] = 1\n",
        "    \n",
        "    # these row-level operations will take a performance hit if we are using coo or csc\n",
        "    normalized_X_train = X_train / X_train_sum\n",
        "    normalized_X_test = X_test / X_test_sum\n",
        "    \n",
        "    # compuite tfidf scores\n",
        "    tfidf_X_train = np.multiply(normalized_X_train,idf_scores)\n",
        "    tfidf_X_test = np.multiply(normalized_X_test,idf_scores)\n",
        "    return tfidf_X_train, tfidf_X_test\n",
        "\n",
        "tfidf_X_train, tfidf_X_test = tfidf_transform(no_rare_X_train, no_rare_X_test)\n",
        "print(\"X_train shape:\",tfidf_X_train.shape)\n",
        "print(\"X_test shape:\",tfidf_X_test.shape)\n",
        "print(\"X_train sample elements:\",tfidf_X_train[0,:5])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wyw0m7KxDZZ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = sklearn.linear_model.LogisticRegression(solver='lbfgs', max_iter=1000).fit(tfidf_X_train, y_train)\n",
        "train_preds = model.predict(tfidf_X_train)\n",
        "test_preds = model.predict(tfidf_X_test)\n",
        "\n",
        "train_accuracy = sklearn.metrics.accuracy_score(y_train,train_preds)\n",
        "test_accuracy = sklearn.metrics.accuracy_score(y_test,test_preds)\n",
        "print(\"Training accuracy:\",train_accuracy,\"Testing accuracy:\",test_accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBMm3224Nx-v",
        "colab_type": "text"
      },
      "source": [
        "- What kinds of words contribute to positve or negative movie review classifications?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SF6sru4VN2ij",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sort coefficients and store their index in the feature list\n",
        "feats_and_coefs = sorted(list(enumerate(model.coef_[0])),key=lambda x:x[1])\n",
        "pos_features = [fc[0] for fc in feats_and_coefs[:15]]\n",
        "neg_features = [fc[0] for fc in feats_and_coefs[-15:]]\n",
        "\n",
        "print(\"Features predictive of positive reviews:\")\n",
        "print(\" \".join([no_rare_index2vocab[idx] for idx in pos_features]))\n",
        "      \n",
        "print(\"Features predictive of negative reviews:\")\n",
        "print(\" \".join([no_rare_index2vocab[idx] for idx in neg_features]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDx1ntIpMxio",
        "colab_type": "text"
      },
      "source": [
        "- This looks reasonable, but can we do better? \n",
        "- Maybe we've reached our potential using just BOW features?\n",
        "    - Feel free to experiment with this more to see if you get boost the performance significantly using other methods!\n",
        "- Let's see what we can do with a simple bag-of-embeddings type of approach using mean-pooling.\n",
        "    - First, we will need to load our embeddings, just like in the previous notebook:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cs2jmoLGW7eY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "emb_path = \"crawl-300d-2M-subword.bin\"\n",
        "fasttext_model = fasttext.load_model(emb_path)\n",
        "print(\"Loaded fasttext embeddings.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tzp5gASGZBG4",
        "colab_type": "text"
      },
      "source": [
        "- Now, we can use mean-pooling to create a new d-dimensional vector for each document.\n",
        "    - d is the dimension of our word embeddings (300 in this case)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCSlkdcrZJzD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_to_embeddings(X, i2v, emb):\n",
        "    # make it coo matrix if it isn't already\n",
        "    X = scipy.sparse.coo_matrix(X)\n",
        "    row2embedding_sum = {}\n",
        "    row2embedding_count = collections.defaultdict(int)\n",
        "    embeddings = []\n",
        "    # iterate through nonzero elements of the coo matrix\n",
        "    # there will be a row, col, and data list the can be indexed with i\n",
        "    for i in range(X.getnnz()):\n",
        "        if X.row[i] not in row2embedding_sum:\n",
        "            row2embedding_sum[X.row[i]] = emb[i2v[X.col[i]]] * X.data[i]\n",
        "        else:\n",
        "            row2embedding_sum[X.row[i]] += emb[i2v[X.col[i]]] * X.data[i]\n",
        "        row2embedding_count[X.row[i]] += X.data[i]\n",
        "    for row, embedding_sum in sorted(row2embedding_sum.items()):\n",
        "        embeddings.append(embedding_sum / row2embedding_count[row])\n",
        "    return np.array(embeddings)\n",
        "\n",
        "emb_X_train = convert_to_embeddings(X_train, index2vocab, fasttext_model)\n",
        "emb_X_test = convert_to_embeddings(X_test, index2vocab, fasttext_model)\n",
        "\n",
        "print(\"X train shape:\", emb_X_train.shape)\n",
        "print(\"X test shape:\", emb_X_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wRRq7enceSw",
        "colab_type": "text"
      },
      "source": [
        "- And checking the results..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-x205pBcczj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = sklearn.linear_model.LogisticRegression(solver='lbfgs', max_iter=1000).fit(emb_X_train, y_train)\n",
        "train_preds = model.predict(emb_X_train)\n",
        "test_preds = model.predict(emb_X_test)\n",
        "\n",
        "train_accuracy = sklearn.metrics.accuracy_score(y_train,train_preds)\n",
        "test_accuracy = sklearn.metrics.accuracy_score(y_test,test_preds)\n",
        "print(\"Training accuracy:\",train_accuracy,\"Testing accuracy:\",test_accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3uOvYCJdljI",
        "colab_type": "text"
      },
      "source": [
        "- Here, bag-of-words beats averaged bag-of-vectors\n",
        "- We can also easily combine the features into a single model, e.g.:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmJUK_3jc4gW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "comb_X_train = np.hstack( (tfidf_X_train, emb_X_train))\n",
        "comb_X_test = np.hstack( (tfidf_X_test, emb_X_test))\n",
        "print(\"X train shape\", comb_X_train.shape)\n",
        "print(\"X test shape\", comb_X_train.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skfRUcqLdw8G",
        "colab_type": "text"
      },
      "source": [
        "- Then check to see if this outperforms the original feature sets (in this case, not so much):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxkqNP39dVZg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = sklearn.linear_model.LogisticRegression(solver='lbfgs', max_iter=1000).fit(comb_X_train, y_train)\n",
        "train_preds = model.predict(comb_X_train)\n",
        "test_preds = model.predict(comb_X_test)\n",
        "\n",
        "train_accuracy = sklearn.metrics.accuracy_score(y_train,train_preds)\n",
        "test_accuracy = sklearn.metrics.accuracy_score(y_test,test_preds)\n",
        "print(\"Training accuracy:\",train_accuracy,\"Testing accuracy:\",test_accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFqVtpXNd6w4",
        "colab_type": "text"
      },
      "source": [
        "- Of course, we could also change our classifier to something else:\n",
        "    - Feel free to try out different models from sklearn: [supervised learning](https://scikit-learn.org/stable/supervised_learning.html).\n",
        "        - You won't have to change the code much! Just swap `linear_model.LogisticRegression()` for some other model to see what happens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWEOfJ9VCqiZ",
        "colab_type": "text"
      },
      "source": [
        "### Putting it together: Upvote prediction on Reddit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9JjzdlPeYHc",
        "colab_type": "text"
      },
      "source": [
        "- Let's load a small sample of reddit data like we did before, but also keep track of the scores and use these for supervision.\n",
        "    - The `score` is computed as $u - d$ where $u=$ *total upvotes* and $d=$ *total downvotes*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdjMbQXaecmT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_reddit_posts_raw = open(\"reddit_2019_05_5K.json\",'r').readlines()\n",
        "reddit_json = [json.loads(post) for post in sample_reddit_posts_raw]\n",
        "\n",
        "texts = []\n",
        "y = []\n",
        "\n",
        "for post in reddit_json:\n",
        "    if len(post['selftext'].strip()) > 100 and post['selftext'] not in [\"[removed]\",\"[deleted]\"]:\n",
        "        text = post['selftext']\n",
        "        score = post['score']\n",
        "        texts.append(text)\n",
        "        y.append(float(score))\n",
        "        \n",
        "X, vocab = docs2matrix_sparse(texts)\n",
        "y = np.array(y)\n",
        "\n",
        "print(\"X shape:\",X.shape)\n",
        "print(\"y shape:\",y.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-psEYdogutR",
        "colab_type": "text"
      },
      "source": [
        "- Let's split our data into train and test:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZslVFiRgt8m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = scipy.sparse.csr_matrix(X)\n",
        "\n",
        "datasize = 800\n",
        "X_train, X_test = X[:datasize,:], X[datasize:,:]\n",
        "y_train, y_test = y[:datasize], y[datasize:]\n",
        "\n",
        "print(\"X_train shape:\",X_train.shape)\n",
        "print(\"X_test shape:\",X_test.shape)\n",
        "print(\"y_train shape:\",y_train.shape)\n",
        "print(\"y_test shape:\",y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hA0qKPNTh5es",
        "colab_type": "text"
      },
      "source": [
        "**Exercise 8:** Upvote prediction\n",
        "\n",
        "- Build a **regression model** to predict reddit upvotes based on text features.\n",
        "    - You won't be able to use classification models for this task\n",
        "        - Check out sklearn's regression models: search for \"regression\" [here](https://scikit-learn.org/stable/supervised_learning.html).\n",
        "            - Hint: you could use the `LinearRegression()` constructor to build a regresion model.\n",
        "    - You can use whichever features you like.\n",
        "        - Consider features outside of just the text. What other post metadata might be helpful?\n",
        "    - Evalute on the test set.\n",
        "    - Feel free to load in *even more* Reddit data if you like and see what kind of performance you can get.\n",
        "    - You should evaluate using [mean absolute error](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html#sklearn.metrics.mean_absolute_error).\n",
        "        - Note that lower scores are better in this case.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Inq4wjbIDkrj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use pieces from previous steps to build a dataset and regression model\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVoIoPCCyNXP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Sample Solution (double-click to view) {display-mode: \"form\"}\n",
        "\n",
        "# nothing fancy, but here is something to give some basic results:\n",
        "model = sklearn.linear_model.LinearRegression().fit(X_train, y_train)\n",
        "train_preds = model.predict(X_train)\n",
        "test_preds = model.predict(X_test)\n",
        "train_error = sklearn.metrics.mean_absolute_error(y_train,train_preds)\n",
        "test_error = sklearn.metrics.mean_absolute_error(y_test,test_preds)\n",
        "\n",
        "print(\"Training error:\",train_error,\"Testing error:\",test_error)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UWnKbgXxYR2",
        "colab_type": "text"
      },
      "source": [
        "## Some resources on deep learning for NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iL6sTd2NzNBX",
        "colab_type": "text"
      },
      "source": [
        "- There is much, much more about deep learning for NLP that has already been covered very nicely elsewhere, and is definitely worth checking out. Just a few pointers to get you started:\n",
        "    - [Basics of Deep Learning](https://colah.github.io/) from Chris Olah's Blog\n",
        "    - [Deep Learning for NLP tutorial](https://pytorch.org/tutorials/beginner/deep_learning_nlp_tutorial.html) by Robert Guthrie\n",
        "    - [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html) by Alexander Rush\n",
        "    - [Transfer Learning in NLP tutorial @ NAACL 2019](https://docs.google.com/presentation/d/1fIhGikFPnb7G5kr58OvYC3GN4io7MznnM0aAgadvJfc/edit) by Ruder, Peters, Swayamdipta, and Wolf\n",
        "- That's it for the **Fundamentals of Text Analysis** Tutorial! Thanks for following along!\n",
        "    "
      ]
    }
  ]
}
