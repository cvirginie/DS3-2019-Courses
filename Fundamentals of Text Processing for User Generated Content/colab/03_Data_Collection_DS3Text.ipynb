{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03_Data_Collection_DS3Text.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/steve-wilson/ds32019/blob/master/03_Data_Collection_DS3Text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APfI_c8B40Vn",
        "colab_type": "text"
      },
      "source": [
        "#Fundamentals of Text Analysis for User Generated Content @ [DS3](https://www.ds3-datascience-polytechnique.fr/)\n",
        "\n",
        "# Part 3: Data Collection\n",
        "\n",
        "[<- Previous: Noisy Text Processing](https://colab.research.google.com/drive/1VlRz-wKYmsQ4gRHb02uLav8RodpvsCNG)\n",
        "\n",
        "[-> Next: Content Analysis](https://colab.research.google.com/drive/1CgXCh1-qxx1FWgSdCuXEKx-bWfAilOnQ)\n",
        "\n",
        "Dates: June 27-28, 2019\n",
        "\n",
        "Facilitator: [Steve Wilson](https://steverw.com)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdTajgZhkGWX",
        "colab_type": "text"
      },
      "source": [
        "## Initial Setup\n",
        "\n",
        "- **Run \"Setup\" below first.**\n",
        "\n",
        "    - This will load libraries and download some resources that we'll use throughout the tutorial.\n",
        "\n",
        "    - You will see a message reading \"Done with setup!\" when this process completes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKVEnPi34qj4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Setup (click the \"run\" button to the left) {display-mode: \"form\"}\n",
        "\n",
        "## Setup ##\n",
        "\n",
        "# imports\n",
        "\n",
        "# built-in Python libraries\n",
        "# -------------------------\n",
        "\n",
        "# For processing the incoming Twitter data\n",
        "import json\n",
        "# os-level utils\n",
        "import os\n",
        "# For downloading web data\n",
        "import requests\n",
        "# For compressing files\n",
        "import zipfile\n",
        "\n",
        "# 3rd party libraries\n",
        "# -------------------\n",
        "\n",
        "# beautiful soup for html parsing\n",
        "!pip install beautifulsoup4\n",
        "import bs4\n",
        "\n",
        "# tweepy for using the Twitter API\n",
        "!pip install tweepy\n",
        "import tweepy\n",
        "\n",
        "\n",
        "# allows downloading of files from colab to your computer\n",
        "from google.colab import files\n",
        "\n",
        "# get sample reddit data\n",
        "if not os.path.exists(\"reddit_2019_05_5K.json\"):\n",
        "    !wget https://raw.githubusercontent.com/steve-wilson/ds32019/master/data/reddit_2019_05_5K.json\n",
        "\n",
        "\n",
        "print()\n",
        "print(\"Done with setup!\")\n",
        "print(\"If you'd like, you can click the (X) button to the left to clear this output.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyPPpRkI6i0n",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "## Data Collection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMYV2R0h6DTN",
        "colab_type": "text"
      },
      "source": [
        "- Here we'll cover a few different sources of user-generated content and provide some examples of how to gather data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZhKTVFO6q2b",
        "colab_type": "text"
      },
      "source": [
        "### Web Scraping and HTML parsing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeBSAsuOHJ3Z",
        "colab_type": "text"
      },
      "source": [
        "- Lots of text data is available directly from web pages.\n",
        "- With the Beautiful Soup library, it's very easy to take some html and extract only the text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TA4bHC-06uXb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "html_content = requests.get(\"http://quotes.toscrape.com/page/1/\").content\n",
        "soup = bs4.BeautifulSoup(html_content,\"html.parser\")\n",
        "print(soup.text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXKZcbEIHjOW",
        "colab_type": "text"
      },
      "source": [
        "- If you want to extract data in a more targeted way, you can navitage the [html document object model](https://www.w3schools.com/whatis/whatis_htmldom.asp) using [Beautiful Soup functions](https://www.crummy.com/software/BeautifulSoup/bs4/doc/), but we won't dive deeply into this for now,\n",
        "- **Important: You should not use this kind of code to just go collect data from any website!**\n",
        "    - Web scaping tools should always check a site's [`robots.txt` file](https://www.robotstxt.org/robotstxt.html), which describes how crawlers, scrapers, indexers, etc., should use the site.\n",
        "        - For example, see [github's robots.txt](https://github.com/robots.txt)\n",
        "    - You should be able to find any site's robots.txt (if there is one) at http://\\<domain\\>/robots.txt for any web \\<domain\\>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Be60X7Eu6zd2",
        "colab_type": "text"
      },
      "source": [
        "### Reddit Corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAWPIB1E_rU9",
        "colab_type": "text"
      },
      "source": [
        "- Reddit is a great source of publicly available user-generated content.\n",
        "- We could scrape Reddit ourselves, but why do that if someone has already (generously) done the heavy lifting?\n",
        "- Reddit user Stuck_in_the_Matrix has compiled and compressed essentially all of Reddit for researchers to download.\n",
        "- [Original submissions corpus](https://www.reddit.com/r/datasets/comments/3mg812/full_reddit_submission_corpus_now_available_2006/) (up to 2015) and [updates](https://files.pushshift.io/reddit/submissions/) (up to 5/2019 at the time of creating this notebook).\n",
        "    - For a smaller file to get started with, take a look at the [daily comments files](https://files.pushshift.io/reddit/comments/daily/).\n",
        "    - To explore more files available, see [this top-level directory](https://files.pushshift.io/reddit/).\n",
        "- Let's explore a small subset of the data from May 2019:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3KftObP6_Rb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read the data that was downloaded during setup\n",
        "# this is the exact format as the full corpus, just truncated to the first 5000 lines\n",
        "sample_reddit_posts_raw = open(\"reddit_2019_05_5K.json\",'r').readlines()\n",
        "print(\"Loaded\",len(sample_reddit_posts_raw),\"reddit posts.\")\n",
        "reddit_json = [json.loads(post) for post in sample_reddit_posts_raw]\n",
        "print(json.dumps(reddit_json[50], sort_keys=True, indent=4))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXB5iK87hLtg",
        "colab_type": "text"
      },
      "source": [
        "- Since the posts are in json format, we used the Python json library to process them.\n",
        "    - This library returns Python dict objects, so we can access them just like we would any other dictionary.\n",
        "- Let's view some of the text content from these posts:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTQbFr4zdsh_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for post in reddit_json[:100]:\n",
        "    if post['selftext'].strip() and post['selftext'] not in [\"[removed]\",\"[deleted]\"]:\n",
        "        print(\"Subreddit:\",post['subreddit'],\"\\nTitle:\",post['title'],\"\\nContent:\", \\\n",
        "              post['selftext'],\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-dIHIU2gs3K",
        "colab_type": "text"
      },
      "source": [
        "- Note that we filtered out posts with no text content.\n",
        "    - Many posts have a non-null \"media\" field, which could contain images, links to youtube, videos, etc.\n",
        "        - These could be worth exploring more, using computer vision to process images/videos and NLP to process linked websites.\n",
        "- That covers the basics of getting Reddit data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxyARJY46vKO",
        "colab_type": "text"
      },
      "source": [
        "### The Twitter API "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkPEhJn_Kga0",
        "colab_type": "text"
      },
      "source": [
        "- Twitter is also known for being an abundant source of publc text data (perhaps even more so than Reddit).\n",
        "- Twitter provides several types of API that can be used to collect anything from tweets to user descriptions to follower networks.\n",
        "    - You can [read all about it here](https://developer.twitter.com/).\n",
        "- For this tutorial, we'll look at using the [standard search API](https://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets.html), which allows us to retreive tweets that contain specific words, phrases, and hashtags.\n",
        "- In the slides, we talked about how to setup a Twitter App and get a API keys.\n",
        "    - You should add your own keys below and then run the code block to set your keys:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdINIx1PLunM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "twitter_API_key = \"\"\n",
        "twitter_API_secret_key = \"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8unaxcDL7gT",
        "colab_type": "text"
      },
      "source": [
        "- Do not share your credentials with anyone!\n",
        "    - You shouldn't hardcode your API keys in code (like above) if you are going to save the file anywhere that is visible to others (like commiting the file to github).\n",
        "        - You can read more about securing your API keys [here](https://developer.twitter.com/en/docs/basics/authentication/guides/securing-keys-and-tokens).\n",
        "     - So, if you plan to save this file in any way, make sure to remove your API keys first.\n",
        "     - If you think your keys have been compromized, you can regenerate them.\n",
        "        - [Apps](https://developer.twitter.com/en/apps) -> Keys and Tokens -> Regenerate\n",
        "- Now, let's see how we can use the [tweepy](https://github.com/tweepy/tweepy) library to collect some tweets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHqPNZl-6yQU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create an auth handler object using the api tokens\n",
        "auth = tweepy.AppAuthHandler(twitter_API_key, twitter_API_secret_key)\n",
        "\n",
        "# tweepy automatically takes care of potential rate limiting issues\n",
        "API = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
        "\n",
        "# let's look for some tweets\n",
        "query = \"#WomensWorldCup\"\n",
        "\n",
        "# count: 100 is the max allowed value for this parameter\n",
        "#     though we might get fewer than that\n",
        "# tweet_mode: Twitter changed the char limit from 140->280, but didn't want\n",
        "#     to break applications expecting 140, so we have to make sure to ask for this.\n",
        "tweets = API.search(q=query, count=100, tweet_mode=\"extended\")\n",
        "\n",
        "print(\"Collected\",len(tweets),\"tweets.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuZytN78mSqB",
        "colab_type": "text"
      },
      "source": [
        "- Great, hopefully you got some tweets! Let's take a look:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYONLu2mmR0L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(json.dumps(tweets[0]._json, sort_keys=True, indent=4))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EMpra6doeC6",
        "colab_type": "text"
      },
      "source": [
        "- Here is the text portion of the tweets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FakwVnGUoR-9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"\\n\\n\".join([tweet.full_text for tweet in tweets]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEWIjJ3iooyB",
        "colab_type": "text"
      },
      "source": [
        "- Things are starting to look a bit more like our examples from the noisy text section.\n",
        "- To make it even easier to collect tweets from page to page, we can use the tweepy Cursor object:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AT4f7mqTw-m_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cursor = tweepy.Cursor(API.search, q=\"#Paris\", tweet_mode=\"extended\")\n",
        "\n",
        "# just get 5 tweets\n",
        "# if not given, will (in theory) retrieve as many matching tweets as possible\n",
        "# (standard search only allows search within previous ~1 week)\n",
        "for tweet in cursor.items(5):\n",
        "    print(tweet.full_text)\n",
        "    print(\"--------\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Lyt5mwD9AH7",
        "colab_type": "text"
      },
      "source": [
        "### Putting it together: building your own corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruX8QSyVqr09",
        "colab_type": "text"
      },
      "source": [
        "**Exercise 4:** Tweet collection\n",
        "\n",
        "- Let's write a function to collect a larger set of tweets related to a query\n",
        "    - If you want to collect data using multiple queries, you can just call this function multiple times, changing the query each time.\n",
        "    - Store the tweets in the file howerever you like\n",
        "        - You will need to write your own parser for this file later on in the tutorial.\n",
        "    - Store whatever information you like about each tweet, but collect the `full_text` at the very least.\n",
        "    - Make sure to check if `limit` is set, and if it is, only collect `limit` tweets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45JYj1e-9O9i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def write_tweets_to_file(API, query, output_filename, limit=5):\n",
        "# ------------- Exercise 3 -------------- #\n",
        "    # gather tweets here, then write to output_filename\n",
        "# ---------------- End ------------------ #\n",
        "\n",
        "# quick test\n",
        "query = \"#twitter\"\n",
        "auth = tweepy.AppAuthHandler(twitter_API_key, twitter_API_secret_key)\n",
        "API = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
        "output_filename = \"test.txt\"\n",
        "write_tweets_to_file(auth, query, output_filename, limit=3)\n",
        "print(\"Wrote this to the file:\",'\\n'+open(output_filename).read())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6wFhuwcrdz7",
        "colab_type": "text"
      },
      "source": [
        "- Now, change the `query` string below to whatever you like, and run the code.\n",
        "    - *Make sure your code above is working before you run this! Otherwise, you may run quite a few queries and hit your rate limit, preventing you from testing your code again for ~15 minutes*\n",
        "    - See [this page](https://developer.twitter.com/en/docs/tweets/search/guides/standard-operators.html) under \"standard search operators\" for details on what kinds of things you can place here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjmMnYD5o8_D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "query = \"#DataScience\"\n",
        "\n",
        "# call the tweet collection function\n",
        "auth = tweepy.AppAuthHandler(twitter_API_key, twitter_API_secret_key)\n",
        "API = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
        "output_filename = \"mytweets.txt\"\n",
        "write_tweets_to_file(API, query, output_filename, 10000)\n",
        "\n",
        "# zip and download\n",
        "output_zip = output_filename + '.zip'\n",
        "with zipfile.ZipFile(output_zip, 'w') as myzip:\n",
        "    myzip.write(output_filename)\n",
        "files.download(output_zip)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjgKtTebEM2w",
        "colab_type": "text"
      },
      "source": [
        "- Note: with some web browsers, the `files.download()` command won't correctly open a dialog window to download the files.\n",
        "    - If this happens, check out the \"Files\" menu on the sidebar\n",
        "        - can be expanded on the left side of this notebook -- click the > button in the top left-corner to unhide the menu.\n",
        "    - You can download your file there (and also upload it when you need it in the next notebook)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjmRRgzmyIXH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Sample Solution (double-click to view) {display-mode: \"form\"}\n",
        "\n",
        "def write_tweets_to_file(api, query, output_filename, limit=10):\n",
        "    cursor = tweepy.Cursor(API.search, q=query, tweet_mode=\"extended\")\n",
        "    with open(output_filename,'w') as out:\n",
        "        for tweet in cursor.items(limit):\n",
        "            # using tags since tweets may have newlines in them\n",
        "            # you may also want to write other information to this file,\n",
        "            # or even the entire json object.\n",
        "            out.write('<TWEET>' + tweet.full_text + '</TWEET>\\n')\n",
        "            \n",
        "# quick test\n",
        "query = \"#twitter\"\n",
        "auth = tweepy.AppAuthHandler(twitter_API_key, twitter_API_secret_key)\n",
        "API = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
        "output_filename = \"test2.txt\"\n",
        "write_tweets_to_file(auth, query, output_filename, limit=3)\n",
        "print(\"Wrote this to the file:\",'\\n'+open(output_filename).read())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FmOf-jZ1F-k",
        "colab_type": "text"
      },
      "source": [
        "- You should now have your own file(s) containing Twitter data!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etUZgi-twrES",
        "colab_type": "text"
      },
      "source": [
        "- [-> Next: Content Analysis](https://colab.research.google.com/drive/1CgXCh1-qxx1FWgSdCuXEKx-bWfAilOnQ)"
      ]
    }
  ]
}
