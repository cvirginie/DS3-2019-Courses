{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preamble\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as ss\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from scipy.optimize import minimize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's start by comparing Laplace and Variational approximations.\n",
    "\n",
    "We'll approximate a mixture of two Cauchy distributions with a normal distribution, using both methods.\n",
    "\n",
    "Our target distribution is\n",
    "\n",
    "$P(\\theta) = \\rho\\left(\\frac{1}{\\pi \\gamma \\left(1+\\left(\\frac{\\theta-\\mu_1}{\\gamma}\\right)^2\\right)}\\right)\n",
    "+ (1-\\rho)\\left(\\frac{1}{\\pi \\gamma \\left(1+\\left(\\frac{\\theta-\\mu_2}{\\gamma}\\right)^2\\right)}\\right)\n",
    "$\n",
    "\n",
    "where $\\mu_1=-2$, $\\mu_2 = 2$, $\\gamma=1$ and $\\rho=0.6$.\n",
    "\n",
    "Below, we'll write a function to evaluate $\\log P(\\theta)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixture_log_pdf(theta, rho, mu1, mu2, gamma):\n",
    "    lp1 = np.log(rho) + ss.cauchy.logpdf(theta, loc=mu1, scale=gamma)\n",
    "    lp2 = np.log(1-rho) + ss.cauchy.logpdf(theta, loc=mu2, scale=gamma)\n",
    "    lp = np.logaddexp(lp1, lp2)\n",
    "    return lp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu1 = -5.\n",
    "mu2 = 5.\n",
    "rho = 0.6\n",
    "gamma = 1.\n",
    "theta = np.linspace(-15, 15, 1000)\n",
    "p_theta = np.exp(mixture_log_pdf(theta, rho=rho, mu1=mu1, mu2=mu2, gamma=gamma))\n",
    "\n",
    "plt.plot(theta, p_theta)\n",
    "plt.xlabel('theta')\n",
    "plt.ylabel('p(theta)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do you think the Laplace solution will end up looking like?\n",
    "\n",
    "Add your expectations here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's the maximizing value of log(P)?\n",
    "\n",
    "You should be able to tell from the distribution, but if not, then you can use the minimize function from scipy.optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_val = # fill in manually, or calculate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We need to compute the second derivative of log(P).\n",
    "\n",
    "Doing it analytically is going to be ugly, because the log doesn't have a nice form\n",
    "\n",
    "(although, is there an approximate log distribution we could take the second derivative of?)\n",
    "\n",
    "So, we're going to use automatic differentiation via TensorFlow. We'll use the `tf.hessians()` function, since later on, we're going to be working with non-scalar data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate hessian using TF/autodiff\n",
    "\n",
    "# first create a variable wrapper for the MAP, for calculating gradients\n",
    "max_theta = tf.Variable([max_val], dtype=tf.float32)\n",
    "\n",
    "# specify distribution\n",
    "target_dist = tfp.distributions.Mixture( # mixture of distributions\n",
    "    cat=tfp.distributions.Categorical(probs=[rho, 1.-rho]), # mixing proportions\n",
    "    components=[\n",
    "        tfp.distributions.Cauchy(loc=mu1, scale=gamma), # first mixture component\n",
    "        tfp.distributions.Cauchy(loc=mu2, scale=gamma)  # second mixture component\n",
    "    ]\n",
    ")\n",
    "\n",
    "# target: -log(P(theta))\n",
    "neg_log_prob = -1. * (target_dist.log_prob(value=max_theta))\n",
    "\n",
    "hess = tf.hessians(neg_log_prob, max_theta)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(fetches=tf.global_variables_initializer())\n",
    "    hess_val = sess.run(fetches=[hess])\n",
    "\n",
    "hess_val = np.squeeze(hess_val)\n",
    "\n",
    "std_est = 1./np.sqrt(hess_val)\n",
    "\n",
    "print('approximate with a Normal distribution with mean {}, std. dev. {}'.format(max_val, std_est))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see how that looks - does it match what you expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_laplace = ss.norm.pdf(theta, max_val, std_est)\n",
    "\n",
    "plt.plot(theta, p_theta, color='black', label='true')\n",
    "plt.plot(theta, est_laplace, color='blue', label='Laplace')\n",
    "plt.xlabel('theta')\n",
    "plt.ylabel('p(theta)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OK, let's find the normal distribution Q that minimizes KL(Q, P)\n",
    "\n",
    "$$\\mathrm{KL}(Q, P) = \\mathbb{E}_q\\log q(\\theta) - \\mathbb{E}_q \\log p(\\theta)$$\n",
    "\n",
    "### Note we're not really doing \"posterior inference\" here, since we don't have data...\n",
    "\n",
    "* In Variational inference, we are looking to minimize $\\mathbb{E}_q\\log q(\\theta) - \\mathbb{E}_q \\log p(\\theta|x)$\n",
    "* This is equivalent to maximizing the ELBO, $\\mathbb{E}_q\\log p(\\theta, x) - E_q \\log q(\\theta)$\n",
    "* Here, we're not doing posterior inference, we're just matching distributions... there is no $x$ so the ELBO is the negative KL\n",
    "\n",
    "\n",
    "### There are built-in methods for calculating the KL in tensorflow_probability. However, to make sure we understand what's going on, we're going to estimate it explicitly using samples from Q. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = tf.Variable(initial_value = 0.) # mean of Q\n",
    "sigma = tf.Variable(initial_value = 1.) # standard deviation of Q\n",
    "learning_rate = 0.01\n",
    "sample_size = 1000\n",
    "max_steps = 1000\n",
    "\n",
    "q_dist = tfp.distributions.Normal(loc=mu, scale=sigma)\n",
    "samples = q_dist.sample(sample_size)\n",
    "\n",
    "# target_dist is as before, I'm re-writing it for completeness\n",
    "target_dist = tfp.distributions.Mixture(\n",
    "    cat=tfp.distributions.Categorical(probs=[rho, 1.-rho]), # mixing proportions\n",
    "    components=[\n",
    "        tfp.distributions.Cauchy(loc=mu1, scale=gamma), # first mixture component\n",
    "        tfp.distributions.Cauchy(loc=mu2, scale=gamma)  # second mixture component\n",
    "    ]\n",
    ")\n",
    "\n",
    "E_log_p = tf.reduce_mean(target_dist.log_prob(samples))\n",
    "E_log_q = tf.reduce_mean(q_dist.log_prob(samples))\n",
    "\n",
    "kl_loss = E_log_q - E_log_p\n",
    "\n",
    "\n",
    "# Optimizer\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(kl_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(fetches=tf.global_variables_initializer())\n",
    "    obs_mu, obs_sigma = sess.run(fetches=[[mu], [sigma]]) # initial values\n",
    "    obs_loss = sess.run(fetches =[kl_loss])\n",
    "    \n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        ## generate samples from current q_dist\n",
    "        sess.run(fetches=samples)\n",
    "        \n",
    "        # gradient step\n",
    "        sess.run(fetches=train_op)\n",
    "        new_mu, new_sigma = sess.run(fetches=[mu, sigma])\n",
    "        \n",
    "        # update loss\n",
    "        new_loss = sess.run(fetches=kl_loss)\n",
    "        loss_diff = np.abs(new_loss - obs_loss[-1])\n",
    "\n",
    "        obs_mu.append(new_mu)\n",
    "        obs_sigma.append(new_sigma)\n",
    "        obs_loss.append(new_loss)\n",
    "        \n",
    "        if (step - 1) % 100 == 0:\n",
    "            print('iteration {}: mu = {}, sigma = {}, loss = {}'.format(step, new_mu, new_sigma, new_loss))\n",
    "            \n",
    "print('Approximate with a normal with mean {}, std. dev. {}'.format(new_mu, new_sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see how that looks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_kl = ss.norm.pdf(theta, new_mu, new_sigma)\n",
    "plt.plot(theta, p_theta, color='black', label='true')\n",
    "plt.plot(theta, est_laplace, color='blue', label='Laplace')\n",
    "plt.plot(theta, est_kl, color='red', label='KL')\n",
    "plt.xlabel('theta')\n",
    "plt.ylabel('p(theta)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's move on to actual approximate Bayesian inference\n",
    "\n",
    "## We're going to start by looking at a very simple linear regression dataset\n",
    "\n",
    "remission.csv looks at cancer remission ('r') in terms of cell length ('LI')\n",
    "\n",
    "Let's start by looking at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/boot/remission.csv\")\n",
    "print(df.head())\n",
    "\n",
    "plt.scatter(df['LI'], df['r'])\n",
    "plt.xlabel('cell length')\n",
    "plt.ylabel('remission')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the relevant variables into numpy arrays\n",
    "\n",
    "X_obs = np.ones((len(df), 2))\n",
    "X_obs[:, 1] = df['LI']\n",
    "y_obs = [int(i == 1) for i in df['r']]\n",
    "y_obs = np.array(y_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We're going to have to evaluate the (unnormalized) posterior. \n",
    "\n",
    "Here, we write a function to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1./(1. + np.exp(-x))\n",
    "\n",
    "def logprob(beta, X, y, mu0 = 0., sig0 = 1., include_prior = True):\n",
    "    ## if include_prior is False, return just the log likelihood\n",
    "    ## This assumes a spherical gaussian prior on beta\n",
    "    \n",
    "    probs = sigmoid(np.dot(X, beta))\n",
    "    lp1 = np.log(probs)\n",
    "    lp0 = np.log(1-probs)\n",
    "    lp = np.sum(lp1*y + lp0*(1-y)) \n",
    "    if include_prior:\n",
    "        lp += np.sum(ss.norm.logpdf(beta, loc=mu0, scale=sig0))\n",
    "    return lp\n",
    "\n",
    "def plot_probs(beta, color='red', label=None):\n",
    "    axes = plt.gca()\n",
    "    x_lim = np.array(axes.get_xlim())\n",
    "    xx = np.linspace(x_lim[0], x_lim[1], 100)\n",
    "    yy = sigmoid(beta[0] + beta[1]*xx)\n",
    "    plt.plot(xx, yy, color=color, label=label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's take a look at the ML estimate and the MAP estimate\n",
    "\n",
    "Fill in the partial code below. Pick whatever values for mu_0 and sigma_0 that you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_hat_ml = minimize((lambda x: -1*logprob(x, X_obs, y_obs, include_prior=False)), x0=np.zeros(2)).x\n",
    "mu0 = # TODO \n",
    "sig0 = # TODO\n",
    "beta_hat_map = # fill in (you can use the max likelihood minimizer as a template... \n",
    "# make sure to include parameters for mu0 and sig0 in the logprob function!)\n",
    "\n",
    "\n",
    "plt.scatter(df['LI'], df['r'], label='data', color='black')\n",
    "plot_probs(beta_hat_ml, color='red', label='MLE estimate')\n",
    "plot_probs(beta_hat_map, color='blue', label='MAP estimate')\n",
    "plt.xlabel('cell length')\n",
    "plt.ylabel('remission probability')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next, let's calculate the Laplace approximation.\n",
    "\n",
    "We already have the MAP. We could calculate the Hessian by hand, or we can use automatic differentiation.\n",
    "\n",
    "Fill in any gaps in the code below\n",
    "\n",
    "### Bonus question: Calculate the Hessian by hand, and compare with the solution below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first create a variable wrapper for the MAP, for calculating gradients\n",
    "max_beta = tf.Variable(np.expand_dims(beta_hat_map, axis=1), dtype=tf.float32)\n",
    "\n",
    "# We're also going to need variable wrappers for the data, since the log posterior is a function of the data\n",
    "X = tf.Variable(X_obs, dtype=tf.float32)\n",
    "y = tf.Variable(np.expand_dims(y_obs, axis=1), dtype=tf.float32)\n",
    "\n",
    "# specify target log distribution: log(prior) + log(likelihood). \n",
    "# (fill in the TODO gaps below)\n",
    "\n",
    "lik_dist = tfp.distributions.Bernoulli(logits=tf.matmul(<TODO>, <TODO>))\n",
    "\n",
    "prior_dist = tfp.distributions.Normal(loc=<TODO>, scale= <TODO>)\n",
    "\n",
    "log_lik = tf.reduce_sum(lik_dist.log_prob(value=y)) # sum of the log likelihoods for each value of y\n",
    "log_prior = #<TODO> # use the log_lik above as a template... what are we evaluating the probability of here?\n",
    "neg_log_prob = -1.0 * (log_lik + log_prior)\n",
    "\n",
    "\n",
    "hess = tf.hessians(neg_log_prob, max_beta)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(fetches=tf.global_variables_initializer())\n",
    "    obs_hess = np.squeeze(sess.run(fetches=hess))\n",
    "    \n",
    "cov_laplace = np.linalg.inv(obs_hess)\n",
    "print('approximate with a Normal distribution with mean {}, covariance \\n{}'.format(beta_hat_map, cov_laplace))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's compare with the true (unnormalized) posterior\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlist = np.linspace(-10, 10, 100)\n",
    "ylist = np.linspace(-10, 10, 100)\n",
    "xgrid, ygrid = np.meshgrid(xlist, ylist)\n",
    "Z1 = [[logprob([bx, by], X_obs, y_obs, mu0=mu0, sig0=sig0) for bx in xlist] for by in ylist]\n",
    "\n",
    "plt.imshow(Z1)\n",
    "plt.title('true posterior')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "Z2 = [[ss.multivariate_normal.logpdf([bx, by], mean=beta_hat_map, cov=cov_laplace) for bx in xlist] \n",
    "                                     for by in ylist]\n",
    "plt.imshow(Z2)\n",
    "plt.title('Laplace approximation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's now try with Variational Bayes\n",
    "\n",
    "We're now working with a posterior, so instead of minimizing the KL directly, we maximize the ELBO\n",
    "\n",
    "We're going to use the tensorflow probability FFNN layers as our distributions. These  incorporate a bias, so unlike previously, we won't include an explicit 1 in our data representation\n",
    "\n",
    "The DenseFlipout layers use an estimator called the Flipout Monte Carlo estimator to estimate expectations - we don't need to do it explicitly like we did before. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by creating an iterator for our data \n",
    "# (not really necessary in such a small example, but including for completeness)\n",
    "\n",
    "def minibatch(x, y, batch_size=None):\n",
    "    if batch_size is None:\n",
    "        batch_size = len(y) # don't minibatch, use whole data\n",
    "    training_dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    training_batches = training_dataset.repeat().batch(batch_size)\n",
    "    training_iterator = tf.data.make_one_shot_iterator(training_batches)\n",
    "    batch_features, batch_labels = training_iterator.get_next()\n",
    "    return batch_features, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## previously, we explicitly represented the bias using a column in X; here, we'll add a bias in Tensorflow\n",
    "\n",
    "## I'm using the whole data in each minibatch (batch_size=None), since the data is small\n",
    "features, labels = minibatch(X_obs[:, [1]], y_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## flipout layers use a Flipout Monte Carlo estimator to estimate expectations\n",
    "    ## The layer is defined as a linear regression (activation = None)\n",
    "layer = tfp.layers.DenseFlipout(\n",
    "    units=1,\n",
    "    activation=None,\n",
    "    kernel_posterior_fn=tfp.layers.default_mean_field_normal_fn(), # defining q for slope\n",
    "    bias_posterior_fn=tfp.layers.default_mean_field_normal_fn()) # defining q for bias\n",
    "logits = layer(features)\n",
    "labels_distribution = tfp.distributions.Bernoulli(logits=logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We need to optimize the ELBO, so need to calculate it (averaged over training data)\n",
    "num_examples = len(y_obs) # size of training dataset\n",
    "\n",
    "neg_log_likelihood = -tf.reduce_mean(input_tensor=labels_distribution.log_prob(labels))\n",
    "kl = sum(layer.losses) / num_examples  ## E_q log(q(theta)) - E_q log(p(theta)) -- scaled to be on appropriate scale\n",
    "elbo_loss = neg_log_likelihood + kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## let's track training accuracy\n",
    "predictions = tf.cast(logits > 0, dtype=tf.int32)\n",
    "accuracy, accuracy_update_op = tf.compat.v1.metrics.accuracy(\n",
    "      labels=labels, predictions=predictions)\n",
    "\n",
    "## set up the optimizer\n",
    "optimizer = tf.train.AdamOptimizer(\n",
    "        learning_rate=0.01)\n",
    "train_op = optimizer.minimize(loss=elbo_loss)\n",
    "\n",
    "init_op = tf.group(tf.global_variables_initializer(),\n",
    "                     tf.local_variables_initializer())\n",
    "\n",
    "\n",
    "max_steps = 2000\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "\n",
    "    # Fit the model to data.\n",
    "    for step in range(max_steps):\n",
    "      _ = sess.run([train_op, accuracy_update_op])\n",
    "      if step % 100 == 0:\n",
    "        \n",
    "        loss_value, accuracy_value = sess.run([elbo_loss, accuracy])\n",
    "        print(\"Step: {:>3d} Loss: {:.3f} Accuracy: {:.3f}\".format(\n",
    "            step, loss_value, accuracy_value))\n",
    "        \n",
    "    final_kernel_posterior = np.squeeze(sess.run(fetches=[layer.kernel_posterior.mean(), \n",
    "                                                          layer.kernel_posterior.stddev()]))\n",
    "    final_bias_posterior = np.squeeze(sess.run(fetches=[layer.bias_posterior.mean(), \n",
    "                                                       layer.bias_posterior.stddev()]))\n",
    "    \n",
    "    \n",
    "vb_mean = [final_bias_posterior[0], final_kernel_posterior[0]]\n",
    "vb_cov = np.diag([final_bias_posterior[1], final_bias_posterior[1]])\n",
    "\n",
    "\n",
    "print('approximate with a Normal distribution with mean {}, covariance \\n{}'.format(vb_mean, vb_cov))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlist = np.linspace(-10, 10, 100)\n",
    "ylist = np.linspace(-10, 10, 100)\n",
    "xgrid, ygrid = np.meshgrid(xlist, ylist)\n",
    "Z1 = [[logprob([bx, by], X_obs, y_obs, mu0=mu0, sig0=sig0) for bx in xlist] for by in ylist]\n",
    "\n",
    "plt.imshow(Z1)\n",
    "plt.title('true posterior')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "Z2 = [[ss.multivariate_normal.logpdf([bx, by], mean=beta_hat_map, cov=cov_laplace) for bx in xlist] \n",
    "                                     for by in ylist]\n",
    "plt.imshow(Z2)\n",
    "plt.title('Laplace approximation')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "Z3 = [[ss.multivariate_normal.logpdf([bx, by], mean=vb_mean, cov=vb_cov) for bx in xlist] \n",
    "                                     for by in ylist]\n",
    "plt.imshow(Z2)\n",
    "plt.title('VB approximation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Bonus question\n",
    "\n",
    "Look back to where we approximated the mixture model by minimizing the KL divergence\n",
    "\n",
    "$$\\mathrm{KL}(Q, P) =\\mathbb{E}_q\\log \\frac{q(\\theta)}{p(\\theta)} =  \\mathbb{E}_q\\log q(\\theta) - \\mathbb{E}_q \\log p(\\theta)$$\n",
    "\n",
    "This is going to be minimized when $q(\\theta)=p(\\theta) \\; \\forall \\theta$.\n",
    "\n",
    "When the distributions don't match, the KL is going to be small if $q(\\theta)$ is small wherever $p(\\theta)$ is small (so the ratio doesn't blow up). \n",
    "\n",
    "We can take the principles of variational inference, and use a different measure of similarity between $Q$ and $P$. For example, rather than looking at $\\mathrm{KL}(Q, P)$, we could look at $KL(P, Q)$ (remember, KL divergence is asymmetric!) \n",
    "\n",
    "* Thinking about the above discussion, what do you think the solution that minimizes $KL(P, Q)$ will look like? Will it have higher or lower variance than the solution that minimizes $KL(Q, P)$?\n",
    "* Try it out! Modify the variational inference code to minimize $KL(P, Q)$ and plot the result. Did it agree with your intuition?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the KL divergence code from earlier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
