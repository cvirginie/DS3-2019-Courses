{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start by simulating some data from a known function, $f=\\mathrm{sin}(x/2)$\n",
    "\n",
    "### We'll assume no noise for now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First plot the function\n",
    "xspace = np.linspace(0, 20, 500)\n",
    "f = np.sin(xspace / 2) \n",
    "plt.plot(xspace, f, color='red', label='true function')\n",
    "\n",
    "# sample some random locations x\n",
    "N = 20\n",
    "x = np.random.uniform(low=0, high=20, size=N)\n",
    "# and, sample some observations\n",
    "y = np.sin(x / 2)\n",
    "plt.scatter(x, y, color='black', label='observations')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We're going to use a Gaussian process to approximate this\n",
    "\n",
    "A Gaussian process is defined in terms of its covariance function. We can choose any kernel function for this. A common choice is the squared exponential:\n",
    "\n",
    "$$k(x, x') = \\alpha^2 \\exp\\left\\{-\\frac{1}{2\\ell^2}(x-x')^2\\right\\}$$\n",
    "\n",
    "The code below constructs a kernel function, and plots a sample with $\\alpha=1$, $\\ell=1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_exponential_kernel(x1, x2, ell=1., alpha=1.):\n",
    "    dx = np.expand_dims(x1, 1) - np.expand_dims(x2, 0)\n",
    "    return (alpha ** 2) * np.exp(-((dx / ell) ** 2) / 2)\n",
    "\n",
    "\n",
    "def plot_samples(xvals, \n",
    "                 num_samples, \n",
    "                 kernel_function=squared_exponential_kernel, \n",
    "                 kernel_args = {'ell': 1., 'alpha': 1.},\n",
    "                 mean_function = None\n",
    "                ):\n",
    "    K = kernel_function(xvals, xvals, **kernel_args)\n",
    "    for i in range(num_samples):\n",
    "        yvals = np.random.multivariate_normal(mean=np.zeros(xvals.shape), cov=K)\n",
    "        plt.plot(xvals,yvals)\n",
    "        \n",
    "## Let's first look at what the kernel looks like\n",
    "\n",
    "ell=1.\n",
    "alpha=1.\n",
    "xs = np.linspace(0, 20, 500)\n",
    "K_se = squared_exponential_kernel(xs, xs, ell=ell, alpha=alpha)\n",
    "\n",
    "plt.imshow(K_se)\n",
    "plt.title('visualization of squared exp. covariance function, ell={}, alpha={}'.format(ell, alpha))\n",
    "plt.show()\n",
    "\n",
    "## Now let's plot some samples\n",
    "num_samples = 5\n",
    "plot_samples(xs, num_samples, kernel_function=squared_exponential_kernel, kernel_args={'ell': ell, 'alpha': alpha})\n",
    "plt.title('samples from GP with squared exp. covariance function, ell={}, alpha={}'.format(ell, alpha))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore some different values of $\\ell$ and $\\alpha$. What effect do they have on the function? \n",
    "\n",
    "$\\ell$ is often referred to as the lengthscale. Why is this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explore!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's also look at a different kernel. \n",
    "\n",
    "Below, implement\n",
    "\n",
    "$k_a(x,x') = \\exp\\left\\{-\\frac{2\\sin^2\\left(\\frac{x-x'}{2}\\right)}{\\ell^2}\\right\\}$\n",
    "\n",
    "and plot for a few values of $\\ell$. What sort of data might this be appropriate for?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_kernel(x1, x2, ell=1.):\n",
    "    # TODO - should return a len(x2) x len(x2) array\n",
    "    \n",
    "ell = # Pick\n",
    "K_new = new_kernel(xs, xs, ell=ell)\n",
    "\n",
    "plt.imshow(K_new)\n",
    "plt.title('visualization of new covariance function, ell={}'.format(ell))\n",
    "plt.show()\n",
    "\n",
    "plot_samples(xs, num_samples, kernel_function=new_kernel, kernel_args={'ell': ell})\n",
    "\n",
    "plt.title('samples from GP with the new covariance function, ell={}'.format(ell))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can create new covariance functions by adding existing functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comb_kernel(x1, x2, ell_1=1., alpha=1., ell_2=1.):\n",
    "    K_se = squared_exponential_kernel(x1, x2, ell=ell_1, alpha=alpha)\n",
    "    K_new = new_kernel(x1, x2, ell=ell_2)\n",
    "    return K_se + K_new\n",
    "\n",
    "ell_1 = # Pick\n",
    "ell_2 = # Pick\n",
    "alpha = # Pick\n",
    "K_comb = comb_kernel(xs, xs, ell_1=ell_1, alpha=alpha, ell_2=ell_2)\n",
    "plt.imshow(K_comb)\n",
    "plt.title('visualization of combined covariance function')\n",
    "plt.show()\n",
    "\n",
    "plot_samples(xs, 5, kernel_function=comb_kernel, kernel_args={'ell_1': ell_1, 'alpha': alpha, 'ell_2': ell_2})\n",
    "plt.title('samples from GP with combined covariance function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's now look at posterior inference\n",
    "\n",
    "We know from the lecture notes that $f(x^*) | x, f(x) \\sim \\mathcal{N}\\left(\\widetilde{m}(x^*), \\widetilde{k}(x^*)\\right) $\n",
    "\n",
    "with $\\widetilde{m}(x^*) = k(x^*, x)k(x,x)^{-1}f(x)$ and $\\widetilde{k}(x^*) = k(x^*, x^*) - k(x^*, x)k(x,x)^{-1}k(x,x*)$.\n",
    "\n",
    "Below, I have started implementing a function to estimate the posterior parameters. I've implemented the mean for you; add in implementation of the covariance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def posterior_params(x_train, y_train, x_pred, \n",
    "                     kernel=squared_exponential_kernel, keyword_args={'ell': 1., 'alpha': 1.}):\n",
    "    k_train_train = kernel(x_train, x_train, **keyword_args)\n",
    "    # we could make this more efficient and stable using Cholesky decompositions:\n",
    "    inv_k_train_train = np.linalg.inv(k_train_train) \n",
    "    \n",
    "    k_pred_train = kernel(x_pred, x_train, **keyword_args)\n",
    "    \n",
    "    m_post = np.dot(k_pred_train, np.dot(inv_k_train_train, y_train))\n",
    "    k_post = #TODO\n",
    "    return m_post, k_post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, sample some values from the posterior, using a squared exponential kernel\n",
    "\n",
    "We'll use our plotting values xs for x_pred.\n",
    "\n",
    "Explore how different kernels and prior parameters effect the posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pick some values for ell and alpha \n",
    "\n",
    "ell = # Pick\n",
    "alpha = # Pick\n",
    "\n",
    "# calculate posterior parameters\n",
    "m_post, k_post = posterior_params(x_train=x, y_train=y, x_pred=xs, \n",
    "                     kernel=squared_exponential_kernel, keyword_args={'ell': ell, 'alpha': alpha})\n",
    "\n",
    "\n",
    "# sample from the posterior\n",
    "num_samples = 5\n",
    "for i in range(num_samples):\n",
    "    ys = np.random.multivariate_normal(mean=m_post, cov=k_post)\n",
    "    plt.plot(xs,ys)\n",
    "    \n",
    "# Add in the actual data\n",
    "plt.scatter(x, y, color='black')\n",
    "plt.title('samples from posterior') \n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can also plot the mean plus/minus 2 standard deviations:\n",
    "\n",
    "y_std = np.diag(k_post) # marginal standard deviations\n",
    "y_upper = m_post + 1.96*y_std # 2 std dev upper bound\n",
    "y_lower = m_post - 1.96*y_std\n",
    "\n",
    "\n",
    "plt.fill_between(xs, y_lower, y_upper, color='grey', alpha=0.3)\n",
    "plt.plot(xs, m_post, 'red')\n",
    "plt.scatter(x, y)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the plots above. Each sampled function goes through the datapoints.\n",
    "\n",
    "What if we don't believe we have exact samples from our function? Let's assume our data are generated according to\n",
    "\n",
    "$$f\\sim \\mbox{GP}(0, \\Sigma) \\qquad \\qquad y_i \\sim \\mbox{Normal}(f(x_i), \\sigma^2)$$\n",
    "\n",
    "\n",
    "If $\\mu \\sim \\mbox{Normal}(\\mu_0, \\Sigma_0)$ and $x\\sim \\mbox{Normal}(\\mu, \\Sigma)$, then marginally, $x\\sim \\mbox{Normal}(\\mu_0, \\Sigma_0 + \\Sigma)$. This means that, if we marginalize out $f$, $y$ is sampled from a Gaussian process.\n",
    "\n",
    "* What is the kernel for the Gaussian process describing the noisy observations?\n",
    "* Below, we'll generate a noisy dataset. Implement this kernel, and plot the mean and 2sd for $y^*$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate noisy data\n",
    "\n",
    "# First plot the function\n",
    "xspace = np.linspace(0, 20, 500)\n",
    "f = np.sin(xspace / 2) \n",
    "plt.plot(xspace, f, color='red', label='true function')\n",
    "\n",
    "# sample some random locations x\n",
    "N = 20\n",
    "\n",
    "x_noisy = np.random.uniform(low=0, high=20, size=N)\n",
    "\n",
    "# and, sample some observations\n",
    "noise_sd = 0.5\n",
    "y_noisy = np.sin(x_noisy / 2) + noise_sd * np.random.randn(N)\n",
    "plt.scatter(x_noisy, y_noisy, color='black', label='observations')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## implement the new kernel, based on the squared exponential kernel\n",
    "\n",
    "def noisy_se_kernel(x1, x2, ell=1., alpha=1., noise_sd=0.5):\n",
    "    k = squared_exponential_kernel(x1, x2, ell=ell, alpha=alpha)\n",
    "    if len(x1) < len(x2):\n",
    "        k[:, :len(x1)] += noise_sd * np.eye(len(x1))\n",
    "    else:\n",
    "        k[:len(x2), :] += noise_sd * np.eye(len(x2))\n",
    "    return k\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now see how it works. \n",
    "\n",
    "We'll use the true noise standard deviation, 0.5, here. In practice, we would optimize our parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ell = # Pick\n",
    "alpha = # Pick\n",
    "\n",
    "\n",
    "m_post, k_post = posterior_params(x_train=x_noisy, y_train=y_noisy, x_pred=xs, \n",
    "                     kernel=noisy_se_kernel, keyword_args={'ell': ell, 'alpha': alpha, 'noise_sd': noise_sd})\n",
    "\n",
    "## Plot the credible bounds\n",
    "\n",
    "y_std = np.diag(k_post) # marginal standard deviations\n",
    "y_upper = m_post + 1.96*y_std # 2 std dev upper bound\n",
    "y_lower = m_post - 1.96*y_std\n",
    "\n",
    "\n",
    "plt.fill_between(xs, y_lower, y_upper, color='grey', alpha=0.3)\n",
    "plt.plot(xs, m_post, 'red')\n",
    "plt.scatter(x_noisy, y_noisy)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "## plot some samples\n",
    "\n",
    "num_samples = 5\n",
    "for i in range(num_samples):\n",
    "    ys = np.random.multivariate_normal(mean=m_post, cov=k_post)\n",
    "    plt.plot(xs,ys)\n",
    "    \n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The samples are really noisy, and the posterior variance is very wide!\n",
    "\n",
    "This is because we are sampling from the posterior over observations $y$, rather than the function $f$.\n",
    "\n",
    "We can also sample from the distribution over $f$. To read how to do so (and much more!) check out the resources at `http://www.gaussianprocess.org` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
