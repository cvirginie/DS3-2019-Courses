{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "# Step 1. Setup Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install recogym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from recogym.envs.session import OrganicSessions\n",
    "\n",
    "from numpy.random.mtrand import RandomState\n",
    "from recogym import Configuration, DefaultContext, Observation\n",
    "from recogym.agents import Agent\n",
    "from recogym.agents import organic_user_count_args, OrganicUserEventCounterAgent\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from recogym.evaluate_agent import verify_agents, plot_verify_agents\n",
    "from recogym.agents import FeatureProvider\n",
    "\n",
    "import math\n",
    "import gym\n",
    "from copy import deepcopy\n",
    "from recogym import env_1_args\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [6, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recogym_configuration(num_products, random_seed=42):\n",
    "    return Configuration({\n",
    "        **env_1_args, \n",
    "        'random_seed': random_seed,\n",
    "        'num_products': num_products,\n",
    "        # 'phi_var': 0.0,\n",
    "        # 'sigma_mu_organic': 0.,\n",
    "        # 'sigma_omega': 0.,\n",
    "        # 'K': 5,\n",
    "        # 'number_of_flips': 5,\n",
    "    })\n",
    "\n",
    "\n",
    "def get_environement(num_products, random_seed=42):\n",
    "    env = gym.make('reco-gym-v1')\n",
    "    env.init_gym(get_recogym_configuration(num_products, random_seed=random_seed).__dict__)\n",
    "    \n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "# Step 2. Generate the training data and derive the raw features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "NUM_USERS = 1000\n",
    "NUM_PRODUCTS = 10\n",
    "\n",
    "organic_counter_agent = OrganicUserEventCounterAgent(Configuration({\n",
    "           **organic_user_count_args,\n",
    "           **get_recogym_configuration(NUM_PRODUCTS).__dict__,\n",
    "           'select_randomly': True,\n",
    "       }))\n",
    "\n",
    "env = get_environement(NUM_PRODUCTS)\n",
    "popularity_policy_logs = env.generate_logs(NUM_USERS, organic_counter_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ProductCountFeatureProvider(FeatureProvider):\n",
    "    \"\"\"This feature provider creates a user state based on viewed product count.\n",
    "    Namely, the feature vector of shape (n_products, ) contains for each product how many times the\n",
    "    user has viewed them organically.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(ProductCountFeatureProvider, self).__init__(config)\n",
    "        self.feature_data = np.zeros((self.config.num_products)).astype(int)\n",
    "\n",
    "    def observe(self, observation):\n",
    "        for session in observation.sessions():\n",
    "            self.feature_data[int(session['v'])] += 1\n",
    "\n",
    "    def features(self, observation):\n",
    "        return self.feature_data.copy()\n",
    "\n",
    "    def reset(self):\n",
    "        self.feature_data[:] = 0\n",
    "\n",
    "\n",
    "def build_rectangular_data(logs, feature_provider):\n",
    "    \"\"\"Create a rectangular feature set from the logged data.\n",
    "    For each taken action, we compute the state in which the user was when the action was taken\n",
    "    \"\"\"\n",
    "    user_states, actions, rewards, proba_actions = [], [], [], []\n",
    "    \n",
    "    current_user = None\n",
    "    for _, row in logs.iterrows():\n",
    "        if current_user != row['u']:\n",
    "            # Use has changed: start a new session and reset user state\n",
    "            current_user = row['u']\n",
    "            sessions = OrganicSessions()\n",
    "            feature_provider.reset()\n",
    "        \n",
    "        context = DefaultContext(row['u'], row['t'])\n",
    "        \n",
    "        if row['z'] == 'organic':\n",
    "            sessions.next(context, row['v'])\n",
    "            \n",
    "        else:\n",
    "            # For each bandit event, generate one observation for the user state, the taken action\n",
    "            # the obtained reward and the used probabilities\n",
    "            feature_provider.observe(Observation(context, sessions))\n",
    "            user_states += [feature_provider.features(None)] \n",
    "            actions += [row['a']]\n",
    "            rewards += [row['c']]\n",
    "            proba_actions += [row['ps']] \n",
    "            \n",
    "            # Start a new organic session\n",
    "            sessions = OrganicSessions()\n",
    "    \n",
    "    return np.array(user_states), np.array(actions).astype(int), np.array(rewards), np.array(proba_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# You can now see data that will be provided to our agents based on logistic regressions\n",
    "count_product_views_feature_provider = ProductCountFeatureProvider(config=get_recogym_configuration(NUM_PRODUCTS))\n",
    "user_states, actions, rewards, proba_actions = \\\n",
    "    build_rectangular_data(popularity_policy_logs, count_product_views_feature_provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "preview_start, preview_size = 500, 3\n",
    "\n",
    "print('user product views count at action time')\n",
    "print(user_states[preview_start:preview_start + preview_size])\n",
    "print('taken actions', actions[preview_start:preview_start + preview_size])\n",
    "print('obtained rewards', rewards[preview_start:preview_start + preview_size])\n",
    "print('probablities of the taken actions', proba_actions[preview_start:preview_start + preview_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each product is shown with a probability proportional to the number of times it has been seen by the user relatively to the others products."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "# Step 3.A. Define and train the bandit likelihood agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "In order to be able to make the link between the state and the actions, we need to create cross-features that show how good a certain pair of state,action is from the pv of predicting pClick (pReward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LikelihoodAgent(Agent):\n",
    "    def __init__(self, feature_provider, use_argmax=False, seed=43):\n",
    "        self.feature_provider = feature_provider\n",
    "        self.use_argmax = use_argmax\n",
    "        self.random_state = RandomState(seed)\n",
    "        self.model = None\n",
    "        \n",
    "    @property\n",
    "    def num_products(self):\n",
    "        return self.feature_provider.config.num_products\n",
    "    \n",
    "    def _create_features(self, user_state, action):\n",
    "        \"\"\"Create the features that are used to estimate the expected reward from the user state.\n",
    "        \"\"\"\n",
    "        features = np.zeros(len(user_state) * self.num_products)\n",
    "        features[action * len(user_state): (action + 1) * len(user_state)] = user_state\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def train(self, logs):\n",
    "        user_states, actions, rewards, proba_actions = build_rectangular_data(logs, self.feature_provider)\n",
    "        \n",
    "        features = np.vstack([\n",
    "            self._create_features(user_state, action) \n",
    "            for user_state, action in zip(user_states, actions)\n",
    "        ])\n",
    "        self.model = LogisticRegression(solver='lbfgs', max_iter=5000)\n",
    "        self.model.fit(features, rewards)\n",
    "    \n",
    "    def _score_products(self, user_state):\n",
    "        all_action_features = np.array([\n",
    "            self._create_features(user_state, action) \n",
    "            for action in range(self.num_products)\n",
    "        ])\n",
    "        return self.model.predict_proba(all_action_features)[:, 1]\n",
    "        \n",
    "    def act(self, observation, reward, done):\n",
    "        \"\"\"Act method returns an action based on current observation and past history\"\"\"\n",
    "        self.feature_provider.observe(observation)        \n",
    "        user_state = self.feature_provider.features(observation)\n",
    "        prob = self._score_products(user_state)\n",
    "        \n",
    "        if not self.use_argmax:\n",
    "            action = self.random_state.choice(self.num_products, p=(prob / np.sum(prob)))\n",
    "            ps = prob[action]\n",
    "            all_ps = prob.copy()\n",
    "        else:\n",
    "            action = np.argmax(prob)\n",
    "            ps = 1.0\n",
    "            all_ps = np.zeros(self.num_products)\n",
    "            all_ps[action] = 1.0\n",
    "      \n",
    "        return {\n",
    "            **super().act(observation, reward, done),\n",
    "            **{\n",
    "                'a': action,\n",
    "                'expected-value': prob[action],\n",
    "                'ps': ps,\n",
    "                'ps-a': all_ps,\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def reset(self):\n",
    "        self.feature_provider.reset()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a look at the feature vector used by the Likelihood agent\n",
    "picked_sample = 500\n",
    "\n",
    "count_product_views_feature_provider = ProductCountFeatureProvider(get_recogym_configuration(NUM_PRODUCTS))\n",
    "likelihood_logreg = LikelihoodAgent(count_product_views_feature_provider)\n",
    "\n",
    "print('User state', user_states[picked_sample])\n",
    "print('action', actions[picked_sample])\n",
    "print('Created cross features')\n",
    "print(likelihood_logreg._create_features(user_states[picked_sample], actions[picked_sample]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "likelihood_logreg = LikelihoodAgent(count_product_views_feature_provider, use_argmax=True)\n",
    "likelihood_logreg.train(popularity_policy_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = verify_agents(env, NUM_USERS, {'likelihood logreg': likelihood_logreg})\n",
    "fig = plot_verify_agents(result)\n",
    "plt.grid(linestyle=\":\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3.B. Dissect likelihood agent behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent(env, num_users, agent):\n",
    "    \"\"\"Small utility function to collect stats about you agent on simulated test traffic\n",
    "    It is really recogym specific, you do not need to look at its internal details\n",
    "    \"\"\"\n",
    "    env = deepcopy(env)\n",
    "    env.agent = agent  \n",
    "    \n",
    "    events = []\n",
    "    for user_id in range(num_users):\n",
    "        env.reset(user_id)\n",
    "        observation, reward, done, _ = env.step(None)\n",
    "\n",
    "        while not done:\n",
    "            for session in observation.sessions():\n",
    "                events += [{**session, 'z': 'organic'}]\n",
    "\n",
    "            action, observation, reward, done, info = env.step_offline(observation, reward, done)\n",
    "            events += [{**action, 'z': 'bandit', 'c': reward}]\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "    events_df = pd.DataFrame().from_dict(events)\n",
    "    ordered_cols = ['t', 'u', 'z', 'v', 'a', 'c', 'ps', 'ps-a']\n",
    "    all_cols = ordered_cols + [col for col in events_df.columns if col not in ordered_cols]\n",
    "    return events_df[all_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test_users = 1000\n",
    "likelihood_logreg_test_logs = run_agent(env, n_test_users, likelihood_logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To force the arms to share the same colors\n",
    "palette = {c: f'C{c}' for c in range(NUM_PRODUCTS)} if NUM_PRODUCTS < 20 else None\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 4), sharey=True)\n",
    "sns.barplot(x=\"a\", y=\"expected-value\", data=likelihood_logreg_test_logs, ax=axes[0], estimator=sum, palette=palette)\n",
    "axes[0].set_title('Expected number of clicks')\n",
    "axes[0].set_xlabel('Selected product')\n",
    "\n",
    "sns.barplot(x=\"a\", y=\"c\", data=likelihood_logreg_test_logs, ax=axes[1], estimator=sum, palette=palette)\n",
    "axes[1].set_title('Obtained number of clicks')\n",
    "axes[1].set_xlabel('Selected product')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of clicks is highly over estimated. This is a good illustration of the optimizer curse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "# Step 4. Define and train the Contextual Bandit agent - VanillaCB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyAgent(LikelihoodAgent):\n",
    "    def __init__(self, feature_provider, use_argmax=False, seed=43):\n",
    "        LikelihoodAgent.__init__(self, feature_provider, use_argmax=use_argmax, seed=seed)\n",
    "    \n",
    "    def _create_features(self, user_state, action):\n",
    "        \"\"\"Create the features that are used to estimate the expected reward from the user state.\n",
    "        \"\"\"\n",
    "        return # ...\n",
    "    \n",
    "    def train(self, reco_log):\n",
    "        user_states, actions, rewards, proba_actions = build_rectangular_data(reco_log, self.feature_provider)\n",
    "        \n",
    "        features = np.vstack([\n",
    "            self._create_features(user_state, action) \n",
    "            for user_state, action in zip(user_states, actions)\n",
    "        ])        \n",
    "        # labels = ...\n",
    "        # weights = ...\n",
    "        \n",
    "        # self.model = ...\n",
    "        # self.model.fit(...)\n",
    "    \n",
    "    def _score_products(self, user_state):\n",
    "        return self.model.predict_proba(self._create_features(user_state, None).reshape(1, -1))[0, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_logreg = PolicyAgent(count_product_views_feature_provider, use_argmax=True)\n",
    "policy_logreg.train(popularity_policy_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = verify_agents(env, NUM_USERS, {\n",
    "    'likelihood logreg': likelihood_logreg, \n",
    "    'policy logreg': policy_logreg,\n",
    "})\n",
    "fig = plot_verify_agents(result)\n",
    "plt.grid(linestyle=\":\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The policy agent achieves way better CTR than the likelihood agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_logreg_test_logs = run_agent(env, 1000, policy_logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 4), sharey=True)\n",
    "sns.countplot(x=\"a\", data=likelihood_logreg_test_logs, ax=axes[0], palette=palette)\n",
    "axes[0].set_title('Overall behavior of likelihood agent')\n",
    "axes[0].set_ylabel('Number of times this product has been shown')\n",
    "\n",
    "sns.countplot(x=\"a\", data=policy_logreg_test_logs, ax=axes[1], palette=palette)\n",
    "axes[1].set_title('Overall behavior of policy agent')\n",
    "axes[1].set_ylabel('Number of times this product has been shown')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two agents behave quite differently, the policy agent focuses on the very best arms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 4), sharey=True)\n",
    "sns.barplot(x=\"a\", y=\"c\", data=likelihood_logreg_test_logs, ax=axes[0], palette=palette)\n",
    "axes[0].set_title('Performances of likelihood agent')\n",
    "axes[0].set_ylabel('CTR')\n",
    "\n",
    "sns.barplot(x=\"a\", y=\"c\", data=policy_logreg_test_logs, ax=axes[1], palette=palette)\n",
    "axes[1].set_title('Performances of policy agent')\n",
    "axes[1].set_ylabel('CTR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While focusing on the very best arms, the policy agent has a way better CTR for almost all products, namely it is better at choosing which product should be shown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5.A. Define and train Contextual Bandit agent on top product embeddings space - OrganicCB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Embeddings from previous notebook\n",
    "def create_embeddings(reco_log, num_products, embedding_size=5):\n",
    "    organic_reco_log = reco_log[reco_log['z'] == 'organic']\n",
    "\n",
    "    n_users = reco_log['u'].nunique()\n",
    "    counts = np.zeros((n_users, num_products))\n",
    "\n",
    "    binarizer = LabelBinarizer().fit(np.arange(num_products))\n",
    "    for u in range(n_users):\n",
    "        binarized_views_of_user = binarizer.transform(organic_reco_log[organic_reco_log['u'] == u]['v'])\n",
    "        counts[u, :] = binarized_views_of_user.sum(axis=0)\n",
    "\n",
    "    counts_above_zero = 1. * (counts > 0)  # above zero counts only\n",
    "    co_counts = np.matmul(counts_above_zero.T, counts_above_zero)\n",
    "\n",
    "    w, v = np.linalg.eig(co_counts)\n",
    "    idx = np.argsort(w)[::-1]\n",
    "    v = np.real(v[:, idx])\n",
    "    w = np.real(w[idx])\n",
    "\n",
    "    wdash = np.zeros_like(w)\n",
    "\n",
    "    wdash[0: embedding_size] = w[0: embedding_size]\n",
    "    embeddings = np.matmul(v, np.sqrt(np.diag(wdash)))[:, 0: embedding_size]  # keep the non-zero components\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyAgentWithEmbeddings(LikelihoodAgent):\n",
    "    def __init__(self, feature_provider, use_argmax=False, seed=43):\n",
    "        LikelihoodAgent.__init__(self, feature_provider, use_argmax=use_argmax, seed=seed)\n",
    "        self.embeddings = None\n",
    "    \n",
    "    def _create_features(self, user_state, action):\n",
    "        \"\"\"Create the features that are used to estimate the expected reward from the user state.\n",
    "        \"\"\"\n",
    "        # User state is the number of time each product has been seen\n",
    "        # We do a weigthed average of these products and use them as features\n",
    "        return # ...\n",
    "    \n",
    "    def train(self, reco_log):\n",
    "        user_states, actions, rewards, proba_actions = build_rectangular_data(reco_log, self.feature_provider)\n",
    "        self.embeddings = # ...\n",
    "        \n",
    "        features = np.vstack([\n",
    "            self._create_features(user_state, action) \n",
    "            for user_state, action in zip(user_states, actions)\n",
    "        ])        \n",
    "        # This is the same as policy agent\n",
    "        # labels = ...\n",
    "        # weights = ...\n",
    "        \n",
    "        # self.model = ...\n",
    "        # self.model.fit(...)\n",
    "    \n",
    "    def _score_products(self, user_state):\n",
    "        return self.model.predict_proba(self._create_features(user_state, None).reshape(1, -1))[0, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "policy_logreg_with_embeddings = PolicyAgentWithEmbeddings(count_product_views_feature_provider, use_argmax=True)\n",
    "policy_logreg_with_embeddings.train(popularity_policy_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = verify_agents(env, NUM_USERS, {\n",
    "    'likelihood logreg': likelihood_logreg, \n",
    "    'policy logreg': policy_logreg,\n",
    "    'policy logreg emb': policy_logreg_with_embeddings,    \n",
    "})\n",
    "fig = plot_verify_agents(result)\n",
    "plt.grid(linestyle=\":\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5.B. Representation of products and users in the embedding space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embedded_user_states_and_products(user_states, product_embeddings, actions):    \n",
    "    # Transform user states\n",
    "    embedded_user_states = [\n",
    "        policy_logreg_with_embeddings._create_features(user_state, None)\n",
    "        for user_state in user_states\n",
    "    ]\n",
    "    \n",
    "    # We now want to represent them with products in two dimensions\n",
    "    tsne = TSNE(n_components=2)\n",
    "    all_vectors_to_project = np.vstack((product_embeddings, embedded_user_states))\n",
    "    \n",
    "    projected_vectors = tsne.fit_transform(all_vectors_to_project)\n",
    "\n",
    "    projected_user_states = projected_vectors[len(product_embeddings):]\n",
    "    projected_products = projected_vectors[: len(product_embeddings)]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.scatter(projected_user_states[:, 0], projected_user_states[:, 1], \n",
    "                    alpha=0.4, c=[f'C{x}' for x in actions], s=10)\n",
    "\n",
    "    for i in range(len(projected_products)):\n",
    "        plt.scatter(projected_products[i, 0], projected_products[i, 1],\n",
    "                   c=f'C{i}', s=500, marker='*', label=f'$p_{{{i}}}$')\n",
    "        \n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_logreg_with_embeddings_logs = env.generate_logs(NUM_USERS, policy_logreg_with_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings on the train logs\n",
    "embeddings = create_embeddings(popularity_policy_logs, NUM_PRODUCTS)\n",
    "\n",
    "# project these embeddings with user states from the test logs\n",
    "user_states, actions, rewards, proba_actions = build_rectangular_data(\n",
    "    policy_logreg_with_embeddings_logs, count_product_views_feature_provider)\n",
    "\n",
    "user_states_with_clicks = user_states[rewards == 1]\n",
    "actions_with_clicks = actions[rewards == 1]\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plot_embedded_user_states_and_products(user_states_with_clicks, embeddings, actions_with_clicks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to the products embeddings built with organic data, we can transfer knowledge the train log to the test log. The user states that have led to clicks in the test log are indeed linked to the embeddings from the train log of these products."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6. KNN Agent for scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this observation, one can derive an agent that exploits this directly and can scale to millions of products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "class KNNAGent(LikelihoodAgent):\n",
    "    def __init__(self, feature_provider, use_argmax=True, seed=43):\n",
    "        LikelihoodAgent.__init__(self, feature_provider, use_argmax=use_argmax, seed=seed)\n",
    "        self.embeddings = None\n",
    "        self.kdd_tree = None\n",
    "    \n",
    "    def _create_features(self, user_state, action):\n",
    "        \"\"\"Create the features that are used to estimate the expected reward from the user state.\n",
    "        \"\"\"\n",
    "        # User state is the number of time each product has been seen\n",
    "        # We do a weigthed average of these products and use them as features\n",
    "        return # ...\n",
    "    \n",
    "    def train(self, reco_log):\n",
    "        self.embeddings = create_embeddings(reco_log, self.num_products)\n",
    "        self.kdd_tree = spatial.KDTree(self.embeddings)\n",
    "    \n",
    "    def _score_products(self, user_state):\n",
    "        # For an easier integration, we \"fake\" scores\n",
    "        action = self.kdd_tree.query(self._create_features(user_state, None))[1]\n",
    "        scores = np.zeros(self.num_products)\n",
    "        scores[action] = 1\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_agent = KNNAGent(count_product_views_feature_provider, use_argmax=True)\n",
    "knn_agent.train(popularity_policy_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = verify_agents(env, NUM_USERS, {\n",
    "    'likelihood logreg': likelihood_logreg, \n",
    "    'policy logreg': policy_logreg,\n",
    "    'policy logreg emb': policy_logreg_with_embeddings,\n",
    "    'knn': knn_agent,\n",
    "})\n",
    "fig = plot_verify_agents(result)\n",
    "plt.grid(linestyle=\":\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
