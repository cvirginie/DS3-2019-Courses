{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.io\n",
    "import scipy.sparse as sp\n",
    "import scipy.linalg as la\n",
    "from scipy.sparse.linalg import svds\n",
    "import matplotlib.pyplot as plt\n",
    "from timeit import timeit\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on [https://ee227c.github.io/code/lecture5.html#projected-gd]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie recommendation using low rank matrix completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are Netflix and have access to the ratings given by users to some movies they saw. Based on this data we want to predict the rating an user would give other movies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data\n",
    "Note that `ratings` is a sparse matrix that in the shape of (num_items, num_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from helpers import load_data, preprocess_data, split_data\n",
    "\n",
    "path_dataset = \"movielens100k.csv\"\n",
    "ratings = load_data(path_dataset, n = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the number of ratings per movie and user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from plots import plot_raw_data\n",
    "\n",
    "num_items_per_user, num_users_per_item = plot_raw_data(ratings)\n",
    "\n",
    "print(\"min # of items per user = {}, min # of users per item = {}.\".format(\n",
    "        min(num_items_per_user), min(num_users_per_item)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into a train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from plots import plot_train_test_data\n",
    "\n",
    "valid_ratings, train, test = split_data(\n",
    "    ratings, num_items_per_user, num_users_per_item, min_num_ratings=1, p_test=0.1)\n",
    "plot_train_test_data(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning low rank matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to minimize the squared error\n",
    "$$\n",
    "\\min_{Y\\in X\\subseteq R^{n\\times m}}\\ \\dfrac{1}{2}\\sum_{(i,j)\\in \\Omega} (Z_{ij} - Y_{ij})^2\n",
    "$$\n",
    "when $\\Omega\\subseteq[n]\\times[m]$ is the set of observed entries from a given matrix $Z$.\n",
    "\n",
    "Since without more assumptions this is a hopeless problem, we assume that the *true* matrix $Y$ is low rank. As a proxy for low rank, we assume that the trace norm of $Y$ is bounded.\n",
    "\n",
    "In this case, our optimization domain is the unit ball of the trace norm (or nuclear norm), which is known to be the convex hull of the rank-1 matrices \n",
    "$$\n",
    "X := \\mathop{conv}(\\mathcal{A}) \\ \\text{ with }\\ \\mathcal{A} := \\Big\\{ u v^\\top \\ \\Big|\\ \\substack{u\\in R^n,\\;||{u}||_2=1\\\\ v\\in R^m,\\;||{v}||_2=1} \\Big\\} \\ .\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost_se(Y,Z):\n",
    "    \"\"\"Compute the objective function on an input matrix Y for the data matrix Z\n",
    "    Assume all (and only) non-zero values of Z are observed\"\"\"\n",
    "    cost = 0\n",
    "    observed_rows,observed_columns = np.nonzero(Z)\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO\n",
    "    # ***************************************************    \n",
    "    raise NotImplementedError    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Baselines "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the global mean to do the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def baseline_global_mean(train, test):\n",
    "    \"\"\"baseline method: use the global mean.\"\"\"\n",
    "    observed_rows,observed_columns = np.nonzero(train)\n",
    "    \n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO\n",
    "    # ***************************************************    \n",
    "    raise NotImplementedError    \n",
    "    test_error = cost_se(Y,test)\n",
    "    print(\"The test error of baseline using global mean: {v}.\".format(v=test_error))\n",
    "\n",
    "baseline_global_mean(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the user means as the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def baseline_user_mean(train, test):\n",
    "    \"\"\"baseline method: use the user means as the prediction.\"\"\"\n",
    "    num_items, num_users = train.shape\n",
    "    \n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO\n",
    "    # ***************************************************    \n",
    "    raise NotImplementedError    \n",
    "\n",
    "    test_error = cost_se(Y,test)\n",
    "    print(\"The test error of baseline using user mean: {v}.\".format(v=test_error))\n",
    "\n",
    "baseline_user_mean(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the item means as the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def baseline_item_mean(train, test):\n",
    "    \"\"\"baseline method: use item means as the prediction.\"\"\"\n",
    "    num_items, num_users = train.shape\n",
    "    \n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO\n",
    "    # ***************************************************    \n",
    "    raise NotImplementedError    \n",
    "    \n",
    "    test_error = cost_se(Y,test)\n",
    "    print(\"The test error of baseline using item mean: {v}.\".format(v=test_error))\n",
    "    \n",
    "baseline_item_mean(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Learn matrix using projected gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_gradient(Y,Z):\n",
    "    \"\"\"Compute the gradient of the objective.\n",
    "    Assume that all non-zero values in Z are observed and so are part of \\Omega\"\"\"\n",
    "    gradient = np.zeros(Y.shape)\n",
    "    observed_rows,observed_columns = np.nonzero(Z)\n",
    "    \n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO\n",
    "    # ***************************************************    \n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def project_onto_simplex(s):\n",
    "    \"\"\"Given a vector s, find its projection onto the unit simplex\"\"\"        \n",
    "   \n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO\n",
    "    # ***************************************************    \n",
    "    raise NotImplementedError    \n",
    "\n",
    "def project_onto_tracenormball(S):\n",
    "    \"\"\"Compute the projection of the matrix S onto the set X (the unit ball of the trace norm)\n",
    "    Hint: use the simplex projection function you wrote above\"\"\"\n",
    "    \n",
    "    \n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO\n",
    "    # ***************************************************    \n",
    "    raise NotImplementedError    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cost of a projecting onto a trace norm ball\n",
    "How does the cost of the projection scale with respect to increasing dimension?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ts = []\n",
    "ns = [100, 200, 400, 600, 800]\n",
    "for n in ns:\n",
    "    f = lambda: project_onto_tracenormball(np.random.normal(0,1,(n, n)))\n",
    "    ts.append(timeit(f, number=1))\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.xlabel('Input dimension')\n",
    "plt.ylabel('Time (s)')\n",
    "plt.title('Cost of nuclear norm projection')\n",
    "plt.plot(ns, ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running projected gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from optimizers import gradient_descent\n",
    "\n",
    "# start from random matrix of nuclear norm 1\n",
    "Y0 = np.random.normal(0,1, train.shape)\n",
    "Y0 = project_onto_tracenormball(Y0)\n",
    "# define the train and test error\n",
    "test_objective = lambda Y: cost_se(Y, test)\n",
    "train_objective = lambda Y: cost_se(Y, train)\n",
    "# run the gradient descent algorithm\n",
    "gradient = lambda Y: compute_gradient(Y, train)\n",
    "Ys, ts = gradient_descent(Y0, [0.2]*10, gradient, project_onto_tracenormball)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Plot the test and train errors vs. number of iterations\"\"\"\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.title('Projected gradient descent error')\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.ylabel('Train error')\n",
    "plt.xlabel('Steps')\n",
    "plt.plot(np.arange(len(Ys)), [train_objective(Y) for Y in Ys], 'ko-')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.ylabel('Test error')\n",
    "plt.xlabel('Steps')\n",
    "plt.plot(np.arange(len(Ys)), [test_objective(Y) for Y in Ys], 'r.-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Plot the test and train errors vs. time\"\"\"\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.title('Projected gradient descent error')\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.ylabel('Train error')\n",
    "plt.xlabel('Time (in sec)')\n",
    "plt.plot(ts, [train_objective(Y) for Y in Ys], 'ko-')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.ylabel('Test error')\n",
    "plt.xlabel('Time (in sec)')\n",
    "plt.plot(ts, [test_objective(Y) for Y in Ys], 'r.-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn matrix using Frank-Wolfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LMO(S):\n",
    "    \"\"\"Compute the linear maximization oracle (LMO) over the unit ball of the trace norm (nuclear norm) for an input S\"\"\"\n",
    "    \n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO\n",
    "    # ***************************************************    \n",
    "    raise NotImplementedError    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cond_grad_update(Y, Z, t):\n",
    "    \"\"\"Compute the Frank-Wolfe update.\n",
    "    Here t is the iteration number, Y is the current point and Z is the observed matrix\"\"\"\n",
    "    gradient = compute_gradient(Y,Z)\n",
    "    V = LMO(-gradient)\n",
    "    \n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO\n",
    "    # ***************************************************    \n",
    "    raise NotImplementedError    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing cost of Projection and Linear Minimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ts1 = []\n",
    "ts2 = []\n",
    "ns = [100, 200, 400, 600, 800]\n",
    "for n in ns:\n",
    "    f = lambda: project_onto_tracenormball(sp.random(n,n))\n",
    "    ts1.append(timeit(f, number=1))\n",
    "    f = lambda: LMO(np.random.normal(0,1,(n, n)))\n",
    "    ts2.append(timeit(f, number=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.xlabel('Input dimension')\n",
    "plt.ylabel('Time (s)')\n",
    "plt.title('Projection vs linear optimization')\n",
    "plt.plot(ns, ts1, label='projection')\n",
    "plt.plot(ns, ts2, label='linear opt')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running Frank-Wolfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from optimizers import frank_wolfe\n",
    "\n",
    "# start from random matrix of nuclear norm 1\n",
    "Y0 = np.random.normal(0,1, train.shape)\n",
    "Y0 = project_onto_tracenormball(Y0)\n",
    "# define the train and test error\n",
    "test_objective = lambda Y: cost_se(Y, test)\n",
    "train_objective = lambda Y: cost_se(Y, train)\n",
    "# run the Frank-Wolfe algorithm\n",
    "update_oracle = lambda Y,k: cond_grad_update(Y, train, k)\n",
    "Ys, ts = frank_wolfe(Y0, update_oracle, num_steps = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Plot the test and train errors vs. number of iterations\"\"\"\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.title('Frank-Wolfe error vs. Number of iterations')\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.ylabel('Train error')\n",
    "plt.xlabel('Steps')\n",
    "plt.plot(np.arange(len(Ys)), [train_objective(Y) for Y in Ys], 'ko-')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.ylabel('Test error')\n",
    "plt.xlabel('Steps')\n",
    "plt.plot(np.arange(len(Ys)), [test_objective(Y) for Y in Ys], 'r.-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Plot the test and train errors vs. time\"\"\"\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.title('Frank-Wolfe error vs. Time')\n",
    "plt.subplot(1,2, 1)\n",
    "plt.ylabel('Train error')\n",
    "plt.xlabel('Time (in sec)')\n",
    "plt.plot(ts, [train_objective(Y) for Y in Ys], 'ko-')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.ylabel('Test error')\n",
    "plt.xlabel('Time (in sec)')\n",
    "plt.plot(ts, [test_objective(Y) for Y in Ys], 'r.-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## A more practical method\n",
    "\n",
    "In practice, to learn a low rank matrix, neither Frank-Wolfe nor Projected Gradient Descent are used. Instead, we formulate a **non-convex** problem which is then solved by SGD. In particular if we want to learn a rank $k$ matrix, $X$ is replaced by two matrices $UV^\\top$ where $U \\in R^{n\\times k}$ and $V \\in R^{m \\times k}$. This means that we never have to store the full matrix $X$ which would take $O(mn)$ space but instead only $O(mk + nk)$ space. Futher, the matrices $U$ and $V$ can be interpreted as *embeddings*. There have been recent theoretical results which prove that this algorithm in fact recovers the correct answer under some assumptions!\n",
    "\n",
    "Refer to this exercise from Machine Learning course (https://github.com/epfml/ML_course/blob/master/labs/ex10/solution/ex10.ipynb) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
