{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from helper import generate_dataset, visualize_one_dataset, visualize_datasets, predict_grid, visualize_predictions\n",
    "from torch.utils import data\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate data\n",
    "\n",
    "We provide a helper function which generates artificial datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 100\n",
    "\n",
    "blobs_train, blobs_test = generate_dataset(\"blobs\", NUM_SAMPLES)\n",
    "moons_train, moons_test = generate_dataset(\"moons\", NUM_SAMPLES)\n",
    "xor_train, xor_test = generate_dataset(\"xor\", NUM_SAMPLES)\n",
    "squares_train, squares_test = generate_dataset(\"bar\", NUM_SAMPLES)\n",
    "\n",
    "# The generate_dataset function returns PyTorch dataset objects\n",
    "type(blobs_train), type(blobs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot the train and the test data sets. Note the differences between the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    (\"Blobs\", blobs_train, blobs_test),\n",
    "    (\"Moons\", moons_train, moons_test),\n",
    "    (\"Bar\", squares_train, squares_test),\n",
    "    (\"XOR\", xor_train, xor_test)\n",
    "]\n",
    "\n",
    "visualize_datasets(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing using PyTorch\n",
    "\n",
    "Write an optimizer in PyTorch by taking using its default SGD class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize(train_data, test_data, model, loss_fn = torch.nn.CrossEntropyLoss(), lr = 0.1):\n",
    "    \"\"\"\n",
    "    Stochastic Gradient Descent optimizer\n",
    "    \n",
    "    :param train_data: torch.utils.data.dataset.Subset\n",
    "    :param test_data: torch.utils.data.dataset.Subset\n",
    "    :param model: torch.nn.Module (see https://pytorch.org/docs/stable/nn.html)\n",
    "    :param loss_fn: torch.nn.modules.loss (see https://pytorch.org/docs/stable/nn.html#id51)\n",
    "    :param lr: float, learning rate\n",
    "    \n",
    "    :return:\n",
    "    - objectives, a list of loss values on the test dataset, collected at the end of each pass over the dataset (epoch)\n",
    "    \"\"\"\n",
    "    # defatult pytorch functions which are useful for loading testing and training data\n",
    "    train_loader = data.DataLoader(train_data, batch_size=10, shuffle=True)\n",
    "    test_loader = data.DataLoader(test_data, batch_size=NUM_SAMPLES)\n",
    "    losses = []\n",
    "        \n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: Define SGD optimizer with learning rate = lr\n",
    "    # HINT: Use torch.optim.SGD and model.parameters()\n",
    "    # ***************************************************\n",
    "    optimizer = ?\n",
    "    \n",
    "    # Run SGD\n",
    "    for epoch in range(1000):\n",
    "        for minibatch, label in train_loader:\n",
    "            \n",
    "            optimizer.zero_grad() # Zeroes the previously computed gradients\n",
    "            \n",
    "            # ***************************************************\n",
    "            # INSERT YOUR CODE HERE\n",
    "            # TODO: prediction on minibatch\n",
    "            # HINT: Use model.forward\n",
    "            # ***************************************************\n",
    "            prediction = ?\n",
    "            \n",
    "            # ***************************************************\n",
    "            # INSERT YOUR CODE HERE\n",
    "            # TODO: compute the loss on prediction\n",
    "            # HINT: Use loss_fn\n",
    "            # ***************************************************\n",
    "            loss = ?\n",
    "            \n",
    "            # ***************************************************\n",
    "            # INSERT YOUR CODE HERE\n",
    "            # TODO: compute the minibatch gradient\n",
    "            # HINT: Use loss.backward!\n",
    "            # ***************************************************\n",
    "            \n",
    "            \n",
    "            # ***************************************************\n",
    "            # INSERT YOUR CODE HERE\n",
    "            # TODO: perform an SGD step\n",
    "            # HINT: Use optimizer.step!\n",
    "            # ***************************************************\n",
    "            \n",
    "            \n",
    "        # Compute the test loss\n",
    "        for minibatch, label in test_loader:\n",
    "            # we let torch know that we dont intend to call .backward\n",
    "            with torch.no_grad():\n",
    "                # ***************************************************\n",
    "                # INSERT YOUR CODE HERE\n",
    "                # TODO: compute the test prediction and test loss\n",
    "                # ***************************************************\n",
    "                loss = ?\n",
    "                \n",
    "                losses.append(loss.item())\n",
    "                \n",
    "                # Print the test loss to monitor progress\n",
    "                if epoch % 100 == 0:\n",
    "                    print(epoch, loss.item())\n",
    "                \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RBF Kernel\n",
    "\n",
    "An RBF kernel is the most commonly used `out of the box` kernel method for non-linear data. Intuitively, an RBF-kernel blurs the training data and uses this for classification i.e. the individual green and blue points above get blurred to make green and blue regions, which are used to make predictions. A critical parameter `sigma` defines the width of this blurring---large `sigma` results in more blurring.\n",
    "\n",
    "See [here](https://github.com/epfml/ML_course/blob/master/lectures/07/lecture07b_kernelRidge.pdf) for more information on the `kernel trick` and [here](https://www.cs.huji.ac.il/~shais/Lectures2014/lecture8.pdf) for an indepth mathematical treatment. Here, we will try develop an intuition for the RBF kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RadialBasisFunction(torch.nn.Module):\n",
    "    def __init__(self, sigma=0.1):\n",
    "        super().__init__()\n",
    "        self.gamma = 1 / (2 * sigma ** 2)\n",
    "        self.num_classes = 2\n",
    "        self.name = 'RBF'\n",
    "    \n",
    "    def init_params(self, train_data):\n",
    "        # data reshaping to do torch broadcasting magic\n",
    "        data_matrix = train_data.dataset.tensors[0][train_data.indices, :]\n",
    "        self.data_matrix = data_matrix.t().view(1, *data_matrix.t().shape)\n",
    "        \n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: initialize parameters alpha to 0\n",
    "        # HINT: use torch.zeros\n",
    "        # ***************************************************\n",
    "        zeros = ?\n",
    "        \n",
    "        self.alpha = torch.nn.Parameter(zeros)\n",
    "\n",
    "    def forward(self, minibatch):\n",
    "        minibatch = minibatch.view(*minibatch.shape, 1)\n",
    "        K = torch.exp(\n",
    "            -self.gamma * torch.sum((self.data_matrix - minibatch) ** 2, dim=1, keepdim=True)\n",
    "        ).squeeze()\n",
    "        return K @ self.alpha\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try you code on the blobs data set. Your test loss should be around 0.01 by the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_model = RadialBasisFunction(0.5)\n",
    "rbf_model.init_params(blobs_train)\n",
    "rbf_blob_losses = optimize(blobs_train, blobs_test, rbf_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the training data points and the predictions made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "predict_grid(rbf_model, ax)\n",
    "visualize_one_dataset(blobs_train, ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results while varying the value of alpha in [0.1, 0.5, 1]. What do you observe? Which is the best value?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_model = RadialBasisFunction(0.1)\n",
    "visualize_predictions(datasets, rbf_model, optimize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "We will create a simple 2 layer neural network using the default functions provided by PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(torch.nn.Sequential):\n",
    "    def __init__(self, hidden_layer_size):\n",
    "        self.name = 'NN'\n",
    "        self.num_classes = 2\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: Define your neural network model with ReLU\n",
    "        # HINT: Use torch.nn.Sequential and torch.nn.ReLU\n",
    "        # ***************************************************\n",
    "        super().__init__(\n",
    "            ?\n",
    "        )\n",
    "        \n",
    "    def init_params(self, train_data):\n",
    "        ''' No need to do anything since it is taken care of by torch.nn.Sequential'''\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results while varying the size of the hidden layer in [20, 200, 1000]\n",
    "\n",
    "What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model = NeuralNetwork(200)\n",
    "visualize_predictions(datasets, nn_model, optimize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also try increase the number of layers. How does this effect the classifier learnt?\n",
    "\n",
    "[This](https://playground.tensorflow.org/) is a cool website where you can play around more with training of neural networks on toy datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
