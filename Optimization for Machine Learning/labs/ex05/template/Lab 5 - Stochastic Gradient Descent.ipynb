{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in previous exercises, we will train a regression model to predict a person's weight from their height. The model is linear regression with one explanatory variable (weight) and an offset. The model is optimized under a least-squares loss. The variable $b$ contains the prediction targets (a vector of the length of the dataset) and the variable $A$ is the data matrix, containing (1) a column of ones and (2) a column with the explanatory variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=False, add_outlier=False)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "b, A = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.shape, A.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least Squares Estimation\n",
    "Least squares estimation is one of the fundamental machine learning algorithms. Given an $ n \\times d $ matrix $A$ and a $ n \\times 1$ vector $b$, the goal is to find a vector $x \\in \\mathbb{R}^d$ which minimizes the objective function $$f(x) = \\frac{1}{2n} \\sum_{i=1}^{n} (a_i^\\top x - b_i)^2 = \\frac{1}{2n} \\|Ax - b\\|^2 $$\n",
    "\n",
    "In this exercise, we will try to fit $x$ using Least Squares Estimation. \n",
    "\n",
    "One can see the function is $\\mu$ strongly convex with $\\mu = \\lambda_{max}(\\nabla^2 f(x))$ and $L$ smooth with $L = \\lambda_{min}(\\nabla^2 f(x)$ everywhere, since here the Hessian matrix is constant, independent of $x$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in the function `minibatch_gradient` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch_gradient(targets_b, data_A, params_x):\n",
    "    \"\"\"\n",
    "    Compute a mini-batch stochastic gradient from a subset of `num_examples` from the dataset.\n",
    "    \n",
    "    :param targets_b: a numpy array of shape (num_examples)\n",
    "    :param data_A: a numpy array of shape (num_examples, num_features)\n",
    "    :param params_x: compute the mini-batch gradient at these parameters, numpy array of shape (num_features)\n",
    "    \n",
    "    :return: gradient: numpy array of shape (num_features)\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: implement stochastic gradient computation. It's same as the gradient descent.\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify empirically for a fixed parameter vector $x$ that the expected value of your `minibatch_gradient` function equals the full gradient. Validating this property for a mini-batch of size 1 is sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***************************************************\n",
    "# INSERT YOUR EXPERIMENT HERE\n",
    "# ***************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, implement stochastic gradient descent for Linear Least Squares, below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient(targets_b, data_A, params_x, batch_size=1):\n",
    "    \"\"\"\n",
    "    Compute a stochastic gradient\n",
    "    \n",
    "    :param targets_b: numpy array of size (num_examples)\n",
    "    :param data_A: numpy array of size (num_examples, num_features)\n",
    "    :param params_x: compute the mini-batch gradient at these parameters, numpy array of shape (num_features)\n",
    "    :param batch_size: integer: number of datapoints to compute the stochastic gradient from\n",
    "    \n",
    "    :return: gradient, numpy array of shape (num_features)\n",
    "    \"\"\"\n",
    "    dataset_size = len(targets_b)\n",
    "    indices = np.random.choice(dataset_size, batch_size, replace=False)\n",
    "    return minibatch_gradient(targets_b[indices], data_A[indices, :], params_x)\n",
    "\n",
    "def stochastic_gradient_descent(\n",
    "        targets_b, \n",
    "        data_A, \n",
    "        initial_x, \n",
    "        batch_size, \n",
    "        max_iters, \n",
    "        initial_learning_rate, \n",
    "        decreasing_learning_rate=False):\n",
    "    \"\"\"\n",
    "    Mini-batch Stochastic Gradient Descent for Linear Least Squares problems.\n",
    "    \n",
    "    :param targets_b: numpy array of size (num_examples)\n",
    "    :param data_A: numpy array of size (num_examples, num_features)\n",
    "    :param initial_x: starting parameters, a numpy array of size (num_features)\n",
    "    :param batch_size: size of the mini-batches\n",
    "    :param max_iters: integer, number of updates to do\n",
    "    :param initial_learning_rate: float\n",
    "    :param decreasing_learning_rate: if set to true, the learning rate should decay linearly to 0 at max_iters\n",
    "    \n",
    "    :return:\n",
    "    - objectives, a list of loss values on the whole dataset, collected at the end of each pass over the dataset (epoch)\n",
    "    - param_states, a list of parameter vectors, collected at the end of each pass over the dataset\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: implement stochastic gradient descent.\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError\n",
    "    return objectives, param_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to test your optimizer with a naive step size with the example code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 200\n",
    "gamma = 0.2\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "x_initial = np.zeros(A.shape[1])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_objectives, sgd_xs = stochastic_gradient_descent(\n",
    "    b, A, x_initial, batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plots import gradient_descent_visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "from grid_search import *\n",
    "\n",
    "# Generate grid data for visualization (parameters to be swept and best combination)\n",
    "grid_x0, grid_x1 = generate_w(num_intervals=10)\n",
    "grid_objectives = grid_search(b, A, grid_x0, grid_x1)\n",
    "loss_star, x0_star, x1_star = get_best_parameters(grid_x0, grid_x1, grid_objectives)\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        sgd_objectives, sgd_xs, grid_objectives, grid_x0, grid_x1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(sgd_xs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Better Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assuming bounded expected stochastic gradients\n",
    "Assume we are moving in a bounded region $\\|x\\| \\leq 25$ containing all iterates (and we assume $\\|x-x^\\star\\| \\leq 25$ as well, for simplicity). By $\\nabla f(x) = \\frac{1}{n}A^\\top (Ax - b)$, one can see that $f$ is Lipschitz over that bounded region, with Lipschitz constant $\\|\\nabla f(x)\\| \\leq \\frac{1}{n} (\\|A^\\top A\\|\\|x\\| + \\|A^\\top Ab\\|)$. We also know that $E\\big[\\|g_t\\|\\big | x_t\\big]\\ = \\nabla f(x)$. So to find B such that  $E\\big[\\|g_t\\|^2\\big]\\leq B^2$, we need to compute the Lipschitz constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 200\n",
    "\n",
    "gamma =  # Fill in a better learning rate  \n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "x_initial = np.zeros(A.shape[1])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_objectives_dec_gamma, sgd_xs_dec_gamma = stochastic_gradient_descent(\n",
    "    b, A, x_initial, batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time visualization with a better learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(sgd_xs_dec_gamma)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD using strong convexity\n",
    "Try doing SGD with a better learning rate using the fact that the objective function is strongly convex.<br>\n",
    "(Hint: you can use a decreasing stepsize parameter gamma in stochastic_gradient_descent.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 200\n",
    "\n",
    "# You may use these results from last week\n",
    "mu = np.linalg.norm(A, -2)**2 / len(A)\n",
    "L = np.linalg.norm(A, 2)**2 / len(A)\n",
    "\n",
    "gamma0 =  # Add your learning rate here\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "x_initial = np.zeros(A.shape[1])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_objectives_dec_gamma, sgd_xs_dec_gamma = stochastic_gradient_descent(\n",
    "    b, A, x_initial, batch_size, max_iters, gamma0, decreasing_learning_rate=True)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evolution of the objective function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to copy your code for the gradient descent implementation from lab03 into the file gradient_descent.py\n",
    "\n",
    "Run the following code to compare SGD (with diffent stepsizes) and gradient descent with respect to the number of gradient computations needed per iteration (remember that gradient descent computes n gradients per iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gradient_descent import gradient_descent\n",
    "x_initial = np.zeros(A.shape[1])\n",
    "gradient_objectives, gradient_xs = gradient_descent(b, A, x_initial, 3, 1/L)\n",
    "\n",
    "sgd_objectives, sgd_xs = stochastic_gradient_descent(\n",
    "    b, A, x_initial, 1, 20000, 0.2)\n",
    "sgd_objectives_dec_gamma, sgd_xs_dec_gamma = stochastic_gradient_descent(\n",
    "    b, A, x_initial, 1, 20000, #fill in the learning rate used above#,\n",
    "    decreasing_gamma = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.xlabel('Number of gradients computed')\n",
    "plt.ylabel('Objective Function')\n",
    "plt.yscale(\"log\")\n",
    "plt.plot(range(0,len(sgd_objectives),100),\n",
    "         sgd_objectives[0:20000:100],'r', label='sgd fixed stepsize')\n",
    "plt.plot(range(0,len(sgd_objectives_dec_gamma),100),\n",
    "         sgd_objectives_dec_gamma[0:20000:100],'b', label='sgd with decreasing step size')\n",
    "plt.plot(range(0,len(gradient_objectives)*10000,10000),\n",
    "         gradient_objectives,'g', label='gradient descent with 1/L stepsize')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading more complex data\n",
    "The data is taken from https://archive.ics.uci.edu/ml/datasets/Concrete+Compressive+Strength "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt(\"concrete_data.csv\",delimiter=\",\")\n",
    "\n",
    "A = data[:,:-1]\n",
    "b = data[:,-1]\n",
    "A, mean_A, std_A = standardize(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.shape, A.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your SGD algorithm on the new dataset. Try different learning rates. Is the outcome very stable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_iters = 1000\n",
    "\n",
    "gamma =  # Fill in a learning rate       \n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "x_initial = np.zeros(A.shape[1])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_objectives, sgd_xs = stochastic_gradient_descent(\n",
    "    b, A, x_initial, batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the Evolution of the Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.xlabel('Number of steps')\n",
    "plt.ylabel('Objective Function')\n",
    "plt.yscale(\"log\")\n",
    "plt.plot(range(len(sgd_objectives)), sgd_objectives,'r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projected Stochastic Gradient Descant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid $x$ becoming too big, we can perform constrained optimization by projecting x onto an $\\text{L}_2$ ball at each iteration, thus limiting the norm of $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the `projection` function below, which projects x onto an L2-ball:\n",
    "(make sure the optimum is inside the l2-ball by choosing an appropriate radius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_ball_radius =   # choose an appropriate radius\n",
    "def projection(x):\n",
    "    \"\"\"project x onto an l2-ball\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute the projection of x onto the l2-ball\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the `projected_stochastic_gradient_descent` function below:<br>\n",
    "(Hint: it is the same as stochastic_gradient_descent but with an extra step in the loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projected_stochastic_gradient_descent(\n",
    "        b, \n",
    "        A, \n",
    "        initial_x, \n",
    "        batch_size, \n",
    "        max_iters, \n",
    "        initial_learning_rate,\n",
    "        decreasing_learning_rate = False, \n",
    "        projection_fn = lambda x: x):\n",
    "\n",
    "    \"\"\"Projected gradient descent.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: implement stochastic gradient descent.\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError\n",
    "    return objectives, xs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your projected SGD function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters = 300\n",
    "\n",
    "gamma =  # Fill in a learning rate      \n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "x_initial = np.zeros(A.shape[1])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "psgd_objectives, psgd_xs = projected_stochastic_gradient_descent(\n",
    "    b, A, x_initial, batch_size, max_iters, gamma,\n",
    "    projection_fn=projection)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.xlabel('Number of steps')\n",
    "plt.ylabel('Objective Function')\n",
    "plt.yscale(\"log\")\n",
    "plt.plot(range(len(psgd_objectives)), psgd_objectives,'r', label='projected sgd')\n",
    "plt.plot(range(len(sgd_objectives)), sgd_objectives,'b', label='sgd')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "widgets": {
   "state": {
    "d2b2c3aea192430e81437f33ba0b0e69": {
     "views": [
      {
       "cell_index": 22
      }
     ]
    },
    "e4a6a7a70ccd42ddb112989c04f2ed3f": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
